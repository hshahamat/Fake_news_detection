{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FxJ9oy_u5cxF"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "data_path = \"drive/My Drive/_data/\"\n",
    "!ls \"drive/My Drive/\"\n",
    "!ls \"drive/My Drive/_data\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('drive/My Drive/')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *#Embedding,Bidirectional,Dense,Conv1D,Flatten,LSTM,GlobalMaxPooling1D,Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2645,
     "status": "ok",
     "timestamp": 1599051653858,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "PL7PPhYD8NRB",
    "outputId": "0ad4b17b-9e4d-4cbb-b119-af984e6ec53a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240\n",
      "2\n",
      "Index(['Statement', 'Label'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statement  Label\n",
       "0  Says the Annies List political group supports ...  False\n",
       "1  When did the decline of coal start? It started...   True\n",
       "2  Hillary Clinton agrees with John McCain \"by vo...   True\n",
       "3  Health care reform legislation is likely to ma...  False\n",
       "4  The economic turnaround started at the end of ...   True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = data_path + \"valid.csv\"\n",
    "df_v = pd.read_csv(data_file)\n",
    "df_v = df_v.dropna() \n",
    "\n",
    "data_file = data_path + \"test.csv\"\n",
    "df_t = pd.read_csv(data_file)\n",
    "df_t = df_t.dropna()\n",
    "\n",
    "data_file = data_path + \"train.csv\"\n",
    "df = pd.read_csv(data_file)\n",
    "df = df.dropna()\n",
    "\n",
    "print(len(df))\n",
    "print(len(df.columns))\n",
    "print(df.columns)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nA_GjGadT_7L"
   },
   "source": [
    "#preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1916,
     "status": "ok",
     "timestamp": 1599051660671,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "hB4MvVAmULEn"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation_number(words):\n",
    "    \"\"\"Remove punctuation & number from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'([^\\s\\w]|_|[0-9])+', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation_number(words)\n",
    "    # words = remove_stopwords(words)\n",
    "    # words = stem_words(words)\n",
    "    # words = lemmatize_verbs(words)\n",
    "    return words\n",
    "\n",
    "def norm_all_sent(_sentences):\n",
    "  for _i,_w in enumerate(_sentences):\n",
    "    # print(_i,_w)\n",
    "    tmp_sentences = _w#_sentences[_w]\n",
    "    tmp_sentences = word_tokenize(tmp_sentences)\n",
    "    tmp_sentences = normalize(tmp_sentences)\n",
    "    tmp_sentences = \" \".join(tmp_sentences)\n",
    "    _sentences[_i] = tmp_sentences\n",
    "  return _sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5776,
     "status": "ok",
     "timestamp": 1599051671452,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "1OzG8TymqjA3",
    "outputId": "72b4ff37-c35b-403f-f22f-0a8b053e0d22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(index_of_words) 12327\n"
     ]
    }
   ],
   "source": [
    "embed_num_dims = 100\n",
    "max_seq_len = 100\n",
    "\n",
    "sentences = df['Statement'].values\n",
    "sentences = norm_all_sent(sentences)\n",
    "Y = df['Label'].values\n",
    "\n",
    "sentences_t = df_t['Statement'].values\n",
    "sentences_t = norm_all_sent(sentences_t)\n",
    "Y_t = df_t['Label'].values\n",
    "\n",
    "sentences_v = df_v['Statement'].values\n",
    "sentences_v = norm_all_sent(sentences_v)\n",
    "Y_v = df_v['Label'].values\n",
    "Y_v = np.where(Y_v=='TRUE',True,False)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)\n",
    "sequence_t = tokenizer.texts_to_sequences(sentences_t)\n",
    "sequence_v = tokenizer.texts_to_sequences(sentences_v)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"len(index_of_words)\",len(index_of_words))\n",
    "\n",
    "padded_seq = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "padded_seq_t = pad_sequences(sequence_t , maxlen = max_seq_len )\n",
    "padded_seq_v = pad_sequences(sequence_v , maxlen = max_seq_len )\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "Y = to_categorical(Y)\n",
    "Y_t = to_categorical(Y_t)\n",
    "Y_v = to_categorical(Y_v)\n",
    "\n",
    "X_train=padded_seq\n",
    "Y_train=Y\n",
    "X_test=padded_seq_t\n",
    "Y_test=Y_t\n",
    "X_val=padded_seq_v\n",
    "Y_val=Y_v\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpxFlNvWJOOz"
   },
   "source": [
    "#CredRank Algorithm\n",
    "##Measuring User Credibility in Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5407,
     "status": "ok",
     "timestamp": 1599053250813,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "NbuVJ6vn6Efv"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import flair\n",
    "except:\n",
    "    !pip install flair\n",
    "import pickle\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "selected_transformer_model=\"bert-base-cased\"\n",
    "embedding = TransformerDocumentEmbeddings(selected_transformer_model)\n",
    "\n",
    "def sentence_embedding(input_S):\n",
    "    _input = Sentence(input_S)\n",
    "    embedding.embed(_input)\n",
    "    _input.get_embedding()\n",
    "    _emb = _input.get_embedding()\n",
    "    _emb = _emb.detach().cpu().numpy()\n",
    "    return _emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8uHoYKHJdsq"
   },
   "outputs": [],
   "source": [
    "# #########Train##############\n",
    "# all_emb = []\n",
    "# for _i,_sen in enumerate(df['Statement']):\n",
    "#     x = _sen\n",
    "#     _emb = sentence_embedding(x)\n",
    "#     print(_i,'=>',_emb.shape)\n",
    "#     all_emb.append(_emb)\n",
    "\n",
    "\n",
    "# outfile = data_path + \"train.npy\"\n",
    "# all_emb = np.array(all_emb)\n",
    "# np.save(outfile, all_emb)\n",
    "# clear_output()\n",
    "\n",
    "# #########Test##############\n",
    "# all_emb = []\n",
    "# for _i,_sen in enumerate(df_t['Statement']):\n",
    "#     x = _sen\n",
    "#     _emb = sentence_embedding(x)\n",
    "#     print(_i,'=>',_emb.shape)\n",
    "#     all_emb.append(_emb)\n",
    "\n",
    "\n",
    "# outfile = data_path + \"test.npy\"\n",
    "# all_emb = np.array(all_emb)\n",
    "# np.save(outfile, all_emb)\n",
    "\n",
    "# #########Valid##############\n",
    "# all_emb = []\n",
    "# for _i,_sen in enumerate(df_v['Statement']):\n",
    "#     x = _sen\n",
    "#     _emb = sentence_embedding(x)\n",
    "#     print(_i,'=>',_emb.shape)\n",
    "#     all_emb.append(_emb)\n",
    "\n",
    "\n",
    "# outfile = data_path + \"valid.npy\"\n",
    "# all_emb = np.array(all_emb)\n",
    "# np.save(outfile, all_emb)\n",
    "\n",
    "# clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1599052004114,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "RAt6h1ZdoCQW",
    "outputId": "770ef118-c995-41c0-b644-d27e5de2d1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_emb_train.shape=> (10240, 768)\n",
      "all_emb_test.shape=> (2551, 768)\n",
      "all_emb_valid.shape=> (2569, 768)\n",
      "tmp_all_emb.shape=> (15360, 768)\n"
     ]
    }
   ],
   "source": [
    "data_file = data_path + \"train.npy\"\n",
    "all_emb = np.load(data_file)\n",
    "print(\"all_emb_train.shape=>\",all_emb.shape)\n",
    "\n",
    "data_file = data_path + \"test.npy\"\n",
    "all_emb_t = np.load(data_file)\n",
    "print(\"all_emb_test.shape=>\",all_emb_t.shape)\n",
    "\n",
    "data_file = data_path + \"valid.npy\"\n",
    "all_emb_v = np.load(data_file)\n",
    "print(\"all_emb_valid.shape=>\",all_emb_v.shape)\n",
    "\n",
    "\n",
    "tmp_all_emb = np.concatenate((all_emb,all_emb_t,all_emb_v))\n",
    "print(\"tmp_all_emb.shape=>\",tmp_all_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 90556,
     "status": "ok",
     "timestamp": 1599052105563,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "3ieYJ4WQdpUT"
   },
   "outputs": [],
   "source": [
    "# # # # from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# # # # from matplotlib import pyplot as plt\n",
    "\n",
    "# # # # linked = linkage(all_emb_t[:10,:], 'single')\n",
    "\n",
    "# # # # plt.figure(figsize=(10, 7))\n",
    "# # # # dendrogram (linked,\n",
    "# # # #             orientation='top',\n",
    "# # # #             distance_sort='descending',\n",
    "# # # #             show_leaf_counts=True)\n",
    "# # # # plt.show()\n",
    "\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster = AgglomerativeClustering(n_clusters=2,affinity='euclidean', linkage='ward')# affinity=\"cosine\", linkage='average')\n",
    "tmp_c=cluster.fit_predict(tmp_all_emb)\n",
    "\n",
    "outfile = data_path + \"clustering_result.npy\"\n",
    "np.save(outfile, tmp_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1043,
     "status": "ok",
     "timestamp": 1599052110812,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "MMVp7AiLDYbs",
    "outputId": "eec95967-bad5-4b0d-bb3d-462bf9a40157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240\n",
      "[0 1]\n",
      "Counter({0: 7974, 1: 2266})\n",
      "2551\n",
      "[0 1]\n",
      "Counter({0: 2000, 1: 551})\n",
      "2569\n",
      "[0 1]\n",
      "Counter({0: 2013, 1: 556})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data_file = data_path + \"clustering_result.npy\"\n",
    "tmp_c = np.load(data_file)\n",
    "\n",
    "\n",
    "train_c = tmp_c[:len(all_emb)]\n",
    "print(len(train_c))\n",
    "print(np.unique(train_c))\n",
    "print(Counter(train_c))\n",
    "# print(Counter(Y_train.argmax(axis=1)))\n",
    "\n",
    "test_c = tmp_c[len(all_emb):len(all_emb)+len(all_emb_t)]\n",
    "print(len(test_c))\n",
    "print(np.unique(test_c))\n",
    "print(Counter(test_c))\n",
    "# print(Counter(Y_train.argmax(axis=1)))\n",
    "\n",
    "valid_c = tmp_c[len(all_emb)+len(all_emb_t):]\n",
    "print(len(valid_c))\n",
    "print(np.unique(valid_c))\n",
    "print(Counter(valid_c))\n",
    "\n",
    "\n",
    "n_of_m = Counter(train_c)\n",
    "cluster_weight = np.zeros(len(n_of_m))\n",
    "_tmp = np.array(list(n_of_m.values()))\n",
    "_for_div = np.sum(np.sqrt(_tmp))\n",
    "for _cluster in n_of_m:\n",
    "    _c_i = np.sqrt(n_of_m[_cluster])\n",
    "    # member_weight = np.sqrt(n_of_m[_cluster])/n_of_m[_cluster]\n",
    "    cluster_weight[_cluster] = (_c_i/_for_div)#*member_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYUTV5m3LV0B"
   },
   "source": [
    "#Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15823,
     "status": "ok",
     "timestamp": 1599052132387,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "Aq0kBzWtufEg",
    "outputId": "977a3f06-2314-4c71-bcfb-3a783d214729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove*.zip\n",
    "# !mv glove*.100d.txt 'drive/My Drive/_MrAnsari/_data'\n",
    "\n",
    "f = open(data_path+'glove.6B.100d.txt')\n",
    "embedd_index = {}\n",
    "for line in f:\n",
    "    val = line.split()\n",
    "    word = val[0]\n",
    "    coff = np.asarray(val[1:],dtype = 'float')\n",
    "    embedd_index[word] = coff\n",
    "\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embedd_index))\n",
    "\n",
    "# a_file = open(data_path+\"data.pkl\", \"wb\")\n",
    "# pickle.dump(embedd_index, a_file)\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L-LYjN4NvkkU"
   },
   "outputs": [],
   "source": [
    "# a_file = open(data_path+\"data.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print('Found %s word vectors.' % len(embedd_index))\n",
    "\n",
    "embedd_index['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1041,
     "status": "ok",
     "timestamp": 1599052134360,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "se70c_6sP_VP",
    "outputId": "f7cf44a3-2b95-4510-9d4e-a8723f0b8d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12328, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(index_of_words) + 1, embed_num_dims))\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "\n",
    "for word,i in index_of_words.items():\n",
    "    temp = embedd_index.get(word)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp\n",
    "        \n",
    "#for plotting\n",
    "        tokens.append(embedding_matrix[i])\n",
    "        labels.append(word)\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9w3uUkwQlDT"
   },
   "outputs": [],
   "source": [
    "#TSNE algorithm used to visualize word embeddings having huge amount (100) dimensions\n",
    "\n",
    "def tsne():\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens[:200])\n",
    "    print(new_values.shape)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16,16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "tsne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1334,
     "status": "ok",
     "timestamp": 1599052819282,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "Sj5nPst4Q_3G",
    "outputId": "a29a0ea1-b3f3-4822-c723-ddfe533d163f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 100, 100)     1232800     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 100, 128)     84480       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           4128        global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            66          dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3)            0           dense_7[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            8           concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,321,482\n",
      "Trainable params: 1,321,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "input1 = Input(shape=(max_seq_len,))\n",
    "input2 = Input(shape=(1,))\n",
    "\n",
    "#Embedding layer before the actaul BLSTM \n",
    "embedd_layer = Embedding(len(index_of_words) + 1 , embed_num_dims , input_length = max_seq_len , weights = [embedding_matrix])(input1)\n",
    "BiLSTM = Bidirectional(LSTM(64 , return_sequences = True , dropout = 0.2 , recurrent_dropout = 0.2))(embedd_layer)\n",
    "GMP = GlobalMaxPooling1D()(BiLSTM)\n",
    "FC1 = Dense(32,activation = 'relu')(GMP)\n",
    "FC1_d = Dropout(0.2)(FC1)\n",
    "FC2 = Dense(2,activation = 'sigmoid')(FC1_d)\n",
    "_newinput = Concatenate()([FC2, input2])\n",
    "FC3 = Dense(2,activation = 'sigmoid')(_newinput)\n",
    "model = Model(inputs=[input1, input2], outputs=FC3)\n",
    "model.summary()\n",
    "\n",
    "# _adam = Adam(lr = 0.01)\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = 'rmsprop' , metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1599052825944,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "Mz9tgyJvwQ9a"
   },
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for _i in range(len(train_c)):\n",
    "    tmp_data = cluster_weight[train_c[_i]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(len(test_c)):\n",
    "    tmp_data = cluster_weight[test_c[_i]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(len(valid_c)):\n",
    "    tmp_data = cluster_weight[valid_c[_i]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 189122,
     "status": "ok",
     "timestamp": 1599053017007,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "zlPOTCytRXp5",
    "outputId": "bc62236b-9aa1-433b-e184-fdd02d8223ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 17s 419ms/step - loss: 0.6843 - accuracy: 0.5612 - val_loss: 0.6801 - val_accuracy: 0.5870\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 17s 419ms/step - loss: 0.6769 - accuracy: 0.5844 - val_loss: 0.6813 - val_accuracy: 0.5753\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 17s 422ms/step - loss: 0.6716 - accuracy: 0.6035 - val_loss: 0.6767 - val_accuracy: 0.5808\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 18s 438ms/step - loss: 0.6679 - accuracy: 0.6078 - val_loss: 0.6728 - val_accuracy: 0.5940\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 17s 435ms/step - loss: 0.6634 - accuracy: 0.6191 - val_loss: 0.6698 - val_accuracy: 0.6072\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 18s 441ms/step - loss: 0.6594 - accuracy: 0.6324 - val_loss: 0.6666 - val_accuracy: 0.6423\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 18s 444ms/step - loss: 0.6537 - accuracy: 0.6535 - val_loss: 0.6635 - val_accuracy: 0.6318\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 18s 447ms/step - loss: 0.6456 - accuracy: 0.6683 - val_loss: 0.6668 - val_accuracy: 0.6228\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 17s 432ms/step - loss: 0.6389 - accuracy: 0.6785 - val_loss: 0.6632 - val_accuracy: 0.6318\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 17s 426ms/step - loss: 0.6306 - accuracy: 0.6896 - val_loss: 0.6583 - val_accuracy: 0.6224\n",
      "80/80 [==============================] - 6s 69ms/step - loss: 0.6634 - accuracy: 0.6025\n",
      "60.25088429450989\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([X_train,new_data],Y_train,epochs = 10 , batch_size = 256, validation_data = ([X_val,new_data_v],Y_val))\n",
    "\n",
    "result = model.evaluate([X_test,new_data_t],Y_test)\n",
    "print(result[1]*100)\n",
    "\n",
    "\n",
    "model_file = data_path + \"LSTM_Model6.h5\"\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6728,
     "status": "ok",
     "timestamp": 1599053029973,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "1qaL4gamT4vl",
    "outputId": "bb101a17-a0d0-4e2c-cb19-0f7ad662b86a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 6s 70ms/step - loss: 0.6634 - accuracy: 0.6025\n",
      "60.25088429450989\n"
     ]
    }
   ],
   "source": [
    "model_file = data_path + \"LSTM_Model6.h5\"\n",
    "model.load_weights(model_file)\n",
    "\n",
    "result = model.evaluate([X_test,new_data_t],Y_test)\n",
    "print(result[1]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34580,
     "status": "ok",
     "timestamp": 1599053083247,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "T6rIGGkh1lyU",
    "outputId": "1fab1721-3ccd-4c22-ec5f-49d8534f1dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Test_Accuracy=> 0.6025088200705606\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "new_data = []\n",
    "for _i in range(len(train_c)):\n",
    "    tmp_data = cluster_weight[train_c[_i]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(len(test_c)):\n",
    "    tmp_data = cluster_weight[test_c[_i]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(len(valid_c)):\n",
    "    tmp_data = cluster_weight[valid_c[_i]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "#######################################\n",
    "#######################################\n",
    "\n",
    "y_pred_v  = model.predict([X_val,new_data_v])\n",
    "# print(\"Valid_Accuracy=>\",np.mean((y_pred_v.argmax(axis=1)==Y_val.argmax(axis=1))))\n",
    "\n",
    "y_pred_t  = model.predict([X_test,new_data_t])\n",
    "# print(\"Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==Y_test.argmax(axis=1))))\n",
    "\n",
    "y_pred  = model.predict([X_train,new_data])\n",
    "# print(\"Train_Accuracy=>\",np.mean((y_pred.argmax(axis=1)==Y_train.argmax(axis=1))))\n",
    "\n",
    "new_data = []\n",
    "for _i in range(y_pred.shape[0]):\n",
    "    tmp_data = [cluster_weight[train_c[_i]],y_pred[_i,1]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(y_pred_t.shape[0]):\n",
    "    tmp_data = [cluster_weight[test_c[_i]],y_pred_t[_i,1]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(y_pred_v.shape[0]):\n",
    "    tmp_data = [cluster_weight[valid_c[_i]],y_pred_v[_i,1]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "\n",
    "\n",
    "mode = 0\n",
    "if mode == 0:\n",
    "    new_train_data = new_data\n",
    "    new_train_label = Y_train.argmax(axis=1)\n",
    "\n",
    "    new_test_data = new_data_t\n",
    "    new_test_label = Y_test.argmax(axis=1)\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==new_test_label)))\n",
    "\n",
    "if mode == 1:\n",
    "    new_train_data = new_data_t[:1500,:]\n",
    "    new_train_label = Y_test.argmax(axis=1)[:1500]\n",
    "\n",
    "    new_test_data = new_data_t[1500:,:]\n",
    "    new_test_label = Y_test.argmax(axis=1)[1500:]\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)[1500:]==new_test_label)))\n",
    "\n",
    "if mode == 2:\n",
    "    new_train_data = new_data_v\n",
    "    new_train_label = Y_val.argmax(axis=1)\n",
    "\n",
    "    new_test_data = new_data_t\n",
    "    new_test_label = Y_test.argmax(axis=1)\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==new_test_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11012,
     "status": "ok",
     "timestamp": 1599053099406,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "aqqVxa0y8SWl",
    "outputId": "02f43b35-090b-44d3-b0ee-435839a4f528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier \n",
      "Confusion matrix(test):\n",
      "[[600 569]\n",
      " [503 879]]\n",
      "Test_Accuracy=> 0.5797726381811055\n",
      "f1-score=>(test):0.6212014134275619\n",
      "ExtraTreesClassifier\n",
      "Confusion matrix(test):\n",
      "[[622 547]\n",
      " [543 839]]\n",
      "Test_Accuracy=> 0.5727165817326538\n",
      "f1-score=>(test):0.6062138728323699\n",
      "SVMClassifier\n",
      "Confusion matrix(test):\n",
      "[[833 336]\n",
      " [689 693]]\n",
      "Test_Accuracy=> 0.5981967855742846\n",
      "f1-score=>(test):0.5748652011613438\n",
      "MLPClassifier\n",
      "Confusion matrix(test):\n",
      "[[814 355]\n",
      " [674 708]]\n",
      "Test_Accuracy=> 0.5966287730301842\n",
      "f1-score=>(test):0.5791411042944786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf1 = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "      min_samples_split=2, random_state=0)\n",
    "clf1.fit(new_train_data, new_train_label)\n",
    "y_pred = clf1.predict(new_test_data)\n",
    "print(\"RandomForestClassifier \")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf2 = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "      min_samples_split=2, random_state=0)\n",
    "clf2.fit(new_train_data, new_train_label)\n",
    "y_pred = clf2.predict(new_test_data)\n",
    "print(\"ExtraTreesClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "clf3 = svm.SVC()\n",
    "clf3.fit(new_train_data, new_train_label)\n",
    "y_pred = clf3.predict(new_test_data)\n",
    "print(\"SVMClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf4 = MLPClassifier(random_state=1, max_iter=300).fit(new_train_data, new_train_label)\n",
    "y_pred = clf4.predict(new_test_data)\n",
    "print(\"MLPClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PaSVdGGA_lWV"
   },
   "source": [
    "#Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1046,
     "status": "ok",
     "timestamp": 1599053309360,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "BbR05VBc_oKL",
    "outputId": "6f8c618a-f379-48ea-9dea-ceaf6e8e2ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_emb_new_sentence.shape=> (1, 768)\n",
      "1:new_sentence=> ['says the annies list political group supports thirdtrimester abortions on demand']\n",
      "2:new_sentence=> [[7, 1, 6734, 1046, 481, 566, 352, 4893, 529, 11, 1480]]\n",
      "3:new_sentence=> (1, 100)\n",
      "_credit_rank.shape=> (1,)\n",
      "******************\n",
      "******************\n",
      "Classificatin Results:\n",
      "Class: false => probability: 0.5351887\n",
      "Class: true => probability: 0.4648114\n",
      "******************\n",
      "******************\n",
      "Final Result for statement==>> says the annies list political group supports thirdtrimester abortions on demand\n",
      "Class: false => probability: 0.5351887\n",
      "Result of second stage of Classification:\n",
      "Predicted Label: true\n"
     ]
    }
   ],
   "source": [
    "new_sentence_ori = ['Says the Annies List political group supports third-trimester abortions on demand.']\n",
    "_emb_new_sentence = sentence_embedding(new_sentence_ori[0]).reshape(1, -1)\n",
    "print(\"_emb_new_sentence.shape=>\",_emb_new_sentence.shape)\n",
    "\n",
    "new_sentence = norm_all_sent(new_sentence_ori)\n",
    "print(\"1:new_sentence=>\",new_sentence)\n",
    "new_sentence = tokenizer.texts_to_sequences(new_sentence)\n",
    "print(\"2:new_sentence=>\",new_sentence)\n",
    "new_sentence = pad_sequences(new_sentence , maxlen = max_seq_len)\n",
    "print(\"3:new_sentence=>\",new_sentence.shape)\n",
    "\n",
    "\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "_dist = euclidean_distances(_emb_new_sentence,all_emb)\n",
    "_nearest_index = _dist.argmin()\n",
    "_credit_rank = cluster_weight[train_c[_nearest_index]]\n",
    "_credit_rank = _credit_rank.reshape(1)\n",
    "print(\"_credit_rank.shape=>\",_credit_rank.shape)\n",
    "\n",
    "y_pred_new_sentence = model.predict([new_sentence,_credit_rank])\n",
    "from scipy.special import softmax\n",
    "y_pred_new_sentence = softmax(y_pred_new_sentence)\n",
    "\n",
    "class_new_sentence = y_pred_new_sentence.argmax()\n",
    "label_dict = ['false','true']\n",
    "\n",
    "print(\"******************\")\n",
    "print(\"******************\")\n",
    "print(\"Classificatin Results:\")\n",
    "for _i in range(len(label_dict)):\n",
    "    print(\"Class:\",label_dict[_i],\"=> probability:\",y_pred_new_sentence[0][_i])\n",
    "print(\"******************\")\n",
    "print(\"******************\")\n",
    "print(\"Final Result for statement==>>\", new_sentence_ori[0])\n",
    "print(\"Class:\",label_dict[class_new_sentence],\"=> probability:\",y_pred_new_sentence[0][class_new_sentence])\n",
    "\n",
    "new_data_for_next_step = np.array([y_pred_new_sentence[0][class_new_sentence],_credit_rank[0]])\n",
    "new_data_for_next_step = new_data_for_next_step.reshape(1, -1)\n",
    "\n",
    "\n",
    "y_pred = clf3.predict(new_data_for_next_step)\n",
    "print(\"Result of second stage of Classification:\")\n",
    "print(\"Predicted Label:\",label_dict[y_pred[0]])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPXBjcu2L9Y2BLMfWX5+jlB",
   "collapsed_sections": [],
   "mount_file_id": "1VEpVfCKAY_ej8Avr4rFHrPUlUmsXJXqL",
   "name": "Text6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

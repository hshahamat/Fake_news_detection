{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FxJ9oy_u5cxF"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "data_path = \"drive/My Drive/_data/\"\n",
    "!ls \"drive/My Drive/\"\n",
    "!ls \"drive/My Drive/_data\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('drive/My Drive/')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *#Embedding,Bidirectional,Dense,Conv1D,Flatten,LSTM,GlobalMaxPooling1D,Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1091,
     "status": "ok",
     "timestamp": 1599047985691,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "PL7PPhYD8NRB",
    "outputId": "ccb6b71e-dbbd-43d5-d32b-e5eba23e8cd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240\n",
      "2\n",
      "Index(['Label', 'Statement'], dtype='object')\n",
      "['false' 'half-true' 'mostly-true' 'true' 'barely-true' 'pants-fire']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label                                          Statement\n",
       "0        false  Says the Annies List political group supports ...\n",
       "1    half-true  When did the decline of coal start? It started...\n",
       "2  mostly-true  Hillary Clinton agrees with John McCain \"by vo...\n",
       "3        false  Health care reform legislation is likely to ma...\n",
       "4    half-true  The economic turnaround started at the end of ..."
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = data_path + \"valid.tsv\"\n",
    "df_v = pd.read_csv(data_file, header=None, sep='\\t', usecols=[1,2], names=['Label', 'Statement'])\n",
    "df_v = df_v.dropna() \n",
    "\n",
    "data_file = data_path + \"test.tsv\"\n",
    "df_t = pd.read_csv(data_file, header=None, sep='\\t', usecols=[1,2], names=['Label', 'Statement'])\n",
    "df_t = df_t.dropna()\n",
    "\n",
    "data_file = data_path + \"train.tsv\"\n",
    "df = pd.read_csv(data_file, header=None, sep='\\t', usecols=[1,2], names=['Label', 'Statement'])\n",
    "df = df.dropna()\n",
    "\n",
    "print(len(df))\n",
    "print(len(df.columns))\n",
    "print(df.columns)\n",
    "print(df.Label.unique())\n",
    "df.head()\n",
    "# df['Statement'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nA_GjGadT_7L"
   },
   "source": [
    "#preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1599045377009,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "hB4MvVAmULEn"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation_number(words):\n",
    "    \"\"\"Remove punctuation & number from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'([^\\s\\w]|_|[0-9])+', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation_number(words)\n",
    "    # words = remove_stopwords(words)\n",
    "    # words = stem_words(words)\n",
    "    # words = lemmatize_verbs(words)\n",
    "    return words\n",
    "\n",
    "def norm_all_sent(_sentences):\n",
    "  for _i,_w in enumerate(_sentences):\n",
    "    # print(_i,_w)\n",
    "    tmp_sentences = _w#_sentences[_w]\n",
    "    tmp_sentences = word_tokenize(tmp_sentences)\n",
    "    tmp_sentences = normalize(tmp_sentences)\n",
    "    tmp_sentences = \" \".join(tmp_sentences)\n",
    "    _sentences[_i] = tmp_sentences\n",
    "  return _sentences\n",
    "\n",
    "def convert_labels(_label):\n",
    "#['false' 'half-true' 'mostly-true' 'true' 'barely-true' 'pants-fire']  \n",
    "    _label[_label=='false']=0\n",
    "    _label[_label=='pants-fire']=1\n",
    "    _label[_label=='half-true']=2\n",
    "    _label[_label=='mostly-true']=3\n",
    "    _label[_label=='barely-true']=4\n",
    "    _label[_label=='true']=5\n",
    "    return _label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4877,
     "status": "ok",
     "timestamp": 1599045389261,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "1OzG8TymqjA3",
    "outputId": "26d00311-f990-4ff1-d839-c651671251a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(index_of_words) 12331\n"
     ]
    }
   ],
   "source": [
    "embed_num_dims = 100\n",
    "max_seq_len = 100\n",
    "\n",
    "sentences = df['Statement'].values\n",
    "sentences = norm_all_sent(sentences)\n",
    "Y = df['Label'].values\n",
    "Y = convert_labels(Y).astype(np.int)\n",
    "\n",
    "\n",
    "sentences_t = df_t['Statement'].values\n",
    "sentences_t = norm_all_sent(sentences_t)\n",
    "Y_t = df_t['Label'].values\n",
    "Y_t = convert_labels(Y_t).astype(np.int)\n",
    "\n",
    "\n",
    "sentences_v = df_v['Statement'].values\n",
    "sentences_v = norm_all_sent(sentences_v)\n",
    "Y_v = df_v['Label'].values\n",
    "Y_v = convert_labels(Y_v).astype(np.int)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)\n",
    "sequence_t = tokenizer.texts_to_sequences(sentences_t)\n",
    "sequence_v = tokenizer.texts_to_sequences(sentences_v)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"len(index_of_words)\",len(index_of_words))\n",
    "\n",
    "padded_seq = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "padded_seq_t = pad_sequences(sequence_t , maxlen = max_seq_len )\n",
    "padded_seq_v = pad_sequences(sequence_v , maxlen = max_seq_len )\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "Y = to_categorical(Y)\n",
    "Y_t = to_categorical(Y_t)\n",
    "Y_v = to_categorical(Y_v)\n",
    "\n",
    "X_train=padded_seq\n",
    "Y_train=Y\n",
    "X_test=padded_seq_t\n",
    "Y_test=Y_t\n",
    "X_val=padded_seq_v\n",
    "Y_val=Y_v\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpxFlNvWJOOz"
   },
   "source": [
    "#CredRank Algorithm\n",
    "##Measuring User Credibility in Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7053,
     "status": "ok",
     "timestamp": 1599046611497,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "gzz6qS5Pmp22"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import flair\n",
    "except:\n",
    "    !pip install flair\n",
    "import pickle\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "selected_transformer_model=\"bert-base-cased\"\n",
    "embedding = TransformerDocumentEmbeddings(selected_transformer_model)\n",
    "\n",
    "def sentence_embedding(input_S):\n",
    "    _input = Sentence(input_S)\n",
    "    embedding.embed(_input)\n",
    "    _input.get_embedding()\n",
    "    _emb = _input.get_embedding()\n",
    "    _emb = _emb.detach().cpu().numpy()\n",
    "    return _emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1243,
     "status": "ok",
     "timestamp": 1599046660762,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "v8uHoYKHJdsq",
    "outputId": "52aeccb3-d8c9-443a-88bc-51ee46a927c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'the department of veterans affairs has a manual out there telling our veterans stuff like are you really of value to your community you know encouraging them to commit suicide'"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #########Train##############\n",
    "# all_emb = []\n",
    "# for _i,_sen in enumerate(df['Statement']):\n",
    "#     x = _sen\n",
    "#     _emb = sentence_embedding(x)\n",
    "#     print(_i,'=>',_emb.shape)\n",
    "#     all_emb.append(_emb)\n",
    "\n",
    "\n",
    "# outfile = data_path + \"train_new.npy\"\n",
    "# all_emb = np.array(all_emb)\n",
    "# np.save(outfile, all_emb)\n",
    "# clear_output()\n",
    "\n",
    "# #########Test##############\n",
    "# all_emb = []\n",
    "# for _i,_sen in enumerate(df_t['Statement']):\n",
    "#     x = _sen\n",
    "#     _emb = sentence_embedding(x)\n",
    "#     print(_i,'=>',_emb.shape)\n",
    "#     all_emb.append(_emb)\n",
    "\n",
    "\n",
    "# outfile = data_path + \"test_new.npy\"\n",
    "# all_emb = np.array(all_emb)\n",
    "# np.save(outfile, all_emb)\n",
    "\n",
    "# #########Valid##############\n",
    "# all_emb = []\n",
    "# for _i,_sen in enumerate(df_v['Statement']):\n",
    "#     x = _sen\n",
    "#     _emb = sentence_embedding(x)\n",
    "#     print(_i,'=>',_emb.shape)\n",
    "#     all_emb.append(_emb)\n",
    "\n",
    "\n",
    "# outfile = data_path + \"valid_new.npy\"\n",
    "# all_emb = np.array(all_emb)\n",
    "# np.save(outfile, all_emb)\n",
    "\n",
    "# clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4072,
     "status": "ok",
     "timestamp": 1599045403665,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "RAt6h1ZdoCQW",
    "outputId": "7ed7d0fa-c843-4fc4-c3e4-78afe010ce9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_emb_train.shape=> (10240, 768)\n",
      "all_emb_test.shape=> (1267, 768)\n",
      "all_emb_valid.shape=> (1284, 768)\n",
      "tmp_all_emb.shape=> (12791, 768)\n"
     ]
    }
   ],
   "source": [
    "data_file = data_path + \"train_new.npy\"\n",
    "all_emb = np.load(data_file)\n",
    "print(\"all_emb_train.shape=>\",all_emb.shape)\n",
    "\n",
    "data_file = data_path + \"test_new.npy\"\n",
    "all_emb_t = np.load(data_file)\n",
    "print(\"all_emb_test.shape=>\",all_emb_t.shape)\n",
    "\n",
    "data_file = data_path + \"valid_new.npy\"\n",
    "all_emb_v = np.load(data_file)\n",
    "print(\"all_emb_valid.shape=>\",all_emb_v.shape)\n",
    "\n",
    "\n",
    "tmp_all_emb = np.concatenate((all_emb,all_emb_t,all_emb_v))\n",
    "print(\"tmp_all_emb.shape=>\",tmp_all_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55599,
     "status": "ok",
     "timestamp": 1599045528309,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "3ieYJ4WQdpUT"
   },
   "outputs": [],
   "source": [
    "# # # # from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# # # # from matplotlib import pyplot as plt\n",
    "\n",
    "# # # # linked = linkage(all_emb_t[:10,:], 'single')\n",
    "\n",
    "# # # # plt.figure(figsize=(10, 7))\n",
    "# # # # dendrogram (linked,\n",
    "# # # #             orientation='top',\n",
    "# # # #             distance_sort='descending',\n",
    "# # # #             show_leaf_counts=True)\n",
    "# # # # plt.show()\n",
    "\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster = AgglomerativeClustering(n_clusters=5,affinity='euclidean', linkage='ward')# affinity=\"cosine\", linkage='average')\n",
    "tmp_c=cluster.fit_predict(tmp_all_emb)\n",
    "\n",
    "outfile = data_path + \"clustering_result_new.npy\"\n",
    "np.save(outfile, tmp_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1407,
     "status": "ok",
     "timestamp": 1599045534571,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "MMVp7AiLDYbs",
    "outputId": "110b72d3-ca85-4454-edaa-59c1ef588f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240\n",
      "[0 1 2 3 4]\n",
      "Counter({0: 3859, 1: 2979, 2: 1652, 4: 1601, 3: 149})\n",
      "1267\n",
      "[0 1 2 3 4]\n",
      "Counter({0: 467, 1: 365, 2: 220, 4: 192, 3: 23})\n",
      "1284\n",
      "[0 1 2 3 4]\n",
      "Counter({0: 514, 1: 365, 2: 200, 4: 193, 3: 12})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data_file = data_path + \"clustering_result_new.npy\"\n",
    "tmp_c = np.load(data_file)\n",
    "\n",
    "\n",
    "train_c = tmp_c[:len(all_emb)]\n",
    "print(len(train_c))\n",
    "print(np.unique(train_c))\n",
    "print(Counter(train_c))\n",
    "# print(Counter(Y_train.argmax(axis=1)))\n",
    "\n",
    "test_c = tmp_c[len(all_emb):len(all_emb)+len(all_emb_t)]\n",
    "print(len(test_c))\n",
    "print(np.unique(test_c))\n",
    "print(Counter(test_c))\n",
    "# print(Counter(Y_train.argmax(axis=1)))\n",
    "\n",
    "valid_c = tmp_c[len(all_emb)+len(all_emb_t):]\n",
    "print(len(valid_c))\n",
    "print(np.unique(valid_c))\n",
    "print(Counter(valid_c))\n",
    "\n",
    "\n",
    "n_of_m = Counter(train_c)\n",
    "cluster_weight = np.zeros(len(n_of_m))\n",
    "_tmp = np.array(list(n_of_m.values()))\n",
    "_for_div = np.sum(np.sqrt(_tmp))\n",
    "for _cluster in n_of_m:\n",
    "    _c_i = np.sqrt(n_of_m[_cluster])\n",
    "    # member_weight = np.sqrt(n_of_m[_cluster])/n_of_m[_cluster]\n",
    "    cluster_weight[_cluster] = (_c_i/_for_div)#*member_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYUTV5m3LV0B"
   },
   "source": [
    "#Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16762,
     "status": "ok",
     "timestamp": 1599045563351,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "Aq0kBzWtufEg",
    "outputId": "2c85f894-67a3-4270-daca-5f5a95d24fdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove*.zip\n",
    "# !mv glove*.100d.txt 'drive/My Drive/_MrAnsari/_data'\n",
    "\n",
    "f = open(data_path+'glove.6B.100d.txt')\n",
    "embedd_index = {}\n",
    "for line in f:\n",
    "    val = line.split()\n",
    "    word = val[0]\n",
    "    coff = np.asarray(val[1:],dtype = 'float')\n",
    "    embedd_index[word] = coff\n",
    "\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embedd_index))\n",
    "\n",
    "# a_file = open(data_path+\"data.pkl\", \"wb\")\n",
    "# pickle.dump(embedd_index, a_file)\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L-LYjN4NvkkU"
   },
   "outputs": [],
   "source": [
    "# a_file = open(data_path+\"data.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print('Found %s word vectors.' % len(embedd_index))\n",
    "\n",
    "embedd_index['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1544,
     "status": "ok",
     "timestamp": 1599045567762,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "se70c_6sP_VP",
    "outputId": "1273cedc-ba92-4de0-a5f3-44c5956b3280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12332, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(index_of_words) + 1, embed_num_dims))\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "\n",
    "for word,i in index_of_words.items():\n",
    "    temp = embedd_index.get(word)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp\n",
    "        \n",
    "#for plotting\n",
    "        tokens.append(embedding_matrix[i])\n",
    "        labels.append(word)\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9w3uUkwQlDT"
   },
   "outputs": [],
   "source": [
    "#TSNE algorithm used to visualize word embeddings having huge amount (100) dimensions\n",
    "\n",
    "def tsne():\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens[:200])\n",
    "    print(new_values.shape)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16,16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "tsne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1664,
     "status": "ok",
     "timestamp": 1599048895614,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "Sj5nPst4Q_3G",
    "outputId": "b71009d6-4759-4c09-e382-af432f0a605b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 100, 100)     1233200     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 100, 128)     84480       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 32)           4128        global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 6)            198         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 7)            0           dense_10[0][0]                   \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 6)            48          concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,322,054\n",
      "Trainable params: 1,322,054\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "input1 = Input(shape=(max_seq_len,))\n",
    "input2 = Input(shape=(1,))\n",
    "\n",
    "#Embedding layer before the actaul BLSTM \n",
    "embedd_layer = Embedding(len(index_of_words) + 1 , embed_num_dims , input_length = max_seq_len , weights = [embedding_matrix])(input1)\n",
    "BiLSTM = Bidirectional(LSTM(64 , return_sequences = True , dropout = 0.2 , recurrent_dropout = 0.2))(embedd_layer)\n",
    "GMP = GlobalMaxPooling1D()(BiLSTM)\n",
    "FC1 = Dense(32,activation = 'relu')(GMP)\n",
    "FC1_d = Dropout(0.2)(FC1)\n",
    "FC2 = Dense(6,activation = 'sigmoid')(FC1_d)\n",
    "_newinput = Concatenate()([FC2, input2])\n",
    "FC3 = Dense(6,activation = 'softmax')(_newinput)\n",
    "model = Model(inputs=[input1, input2], outputs=FC3)\n",
    "model.summary()\n",
    "\n",
    "_adam = Adam(lr = 0.01)\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = _adam , metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1599048909702,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "Mz9tgyJvwQ9a"
   },
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for _i in range(len(train_c)):\n",
    "    tmp_data = cluster_weight[train_c[_i]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(len(test_c)):\n",
    "    tmp_data = cluster_weight[test_c[_i]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(len(valid_c)):\n",
    "    tmp_data = cluster_weight[valid_c[_i]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 92153,
     "status": "ok",
     "timestamp": 1599049004519,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "zlPOTCytRXp5",
    "outputId": "ce517bf7-3485-4894-ab8f-90f17a18a854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 17s 418ms/step - loss: 1.7704 - accuracy: 0.2036 - val_loss: 1.7666 - val_accuracy: 0.1931\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 16s 406ms/step - loss: 1.7578 - accuracy: 0.2062 - val_loss: 1.7627 - val_accuracy: 0.1970\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 16s 409ms/step - loss: 1.7577 - accuracy: 0.2066 - val_loss: 1.7662 - val_accuracy: 0.1931\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 16s 406ms/step - loss: 1.7580 - accuracy: 0.2089 - val_loss: 1.7650 - val_accuracy: 0.1908\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 16s 406ms/step - loss: 1.7573 - accuracy: 0.2094 - val_loss: 1.7630 - val_accuracy: 0.1947\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.7501 - accuracy: 0.2115\n",
      "21.152327954769135\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([X_train,new_data],Y_train,epochs = 50 , batch_size = 256, validation_data = ([X_val,new_data_v],Y_val))\n",
    "\n",
    "result = model.evaluate([X_test,new_data_t],Y_test)\n",
    "print(result[1]*100)\n",
    "\n",
    "model_file = data_path + \"LSTM_Model6_new.h5\"\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5278,
     "status": "ok",
     "timestamp": 1599049220256,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "1qaL4gamT4vl",
    "outputId": "3306c590-8a48-4c3a-ff77-abe4562b2d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 3s 84ms/step - loss: 1.7501 - accuracy: 0.2115\n",
      "21.152327954769135\n"
     ]
    }
   ],
   "source": [
    "model_file = data_path + \"LSTM_Model6_new.h5\"\n",
    "model.load_weights(model_file)\n",
    "\n",
    "result = model.evaluate([X_test,new_data_t],Y_test)\n",
    "print(result[1]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28478,
     "status": "ok",
     "timestamp": 1599049252866,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "T6rIGGkh1lyU",
    "outputId": "4605d309-9f24-4a86-b89f-ecdcf3cc5a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Test_Accuracy=> 0.21152328334648776\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "new_data = []\n",
    "for _i in range(len(train_c)):\n",
    "    tmp_data = cluster_weight[train_c[_i]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(len(test_c)):\n",
    "    tmp_data = cluster_weight[test_c[_i]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(len(valid_c)):\n",
    "    tmp_data = cluster_weight[valid_c[_i]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "#######################################\n",
    "#######################################\n",
    "\n",
    "y_pred_v  = model.predict([X_val,new_data_v])\n",
    "# print(\"Valid_Accuracy=>\",np.mean((y_pred_v.argmax(axis=1)==Y_val.argmax(axis=1))))\n",
    "\n",
    "y_pred_t  = model.predict([X_test,new_data_t])\n",
    "# print(\"Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==Y_test.argmax(axis=1))))\n",
    "\n",
    "y_pred  = model.predict([X_train,new_data])\n",
    "# print(\"Train_Accuracy=>\",np.mean((y_pred.argmax(axis=1)==Y_train.argmax(axis=1))))\n",
    "\n",
    "new_data = []\n",
    "for _i in range(y_pred.shape[0]):\n",
    "    tmp_data = [cluster_weight[train_c[_i]],y_pred[_i,1]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(y_pred_t.shape[0]):\n",
    "    tmp_data = [cluster_weight[test_c[_i]],y_pred_t[_i,1]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(y_pred_v.shape[0]):\n",
    "    tmp_data = [cluster_weight[valid_c[_i]],y_pred_v[_i,1]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "\n",
    "\n",
    "mode = 2\n",
    "if mode == 0:\n",
    "    new_train_data = new_data\n",
    "    new_train_label = Y_train.argmax(axis=1)\n",
    "\n",
    "    new_test_data = new_data_t\n",
    "    new_test_label = Y_test.argmax(axis=1)\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==new_test_label)))\n",
    "\n",
    "if mode == 1:\n",
    "    new_train_data = new_data_t[:1500,:]\n",
    "    new_train_label = Y_test.argmax(axis=1)[:1500]\n",
    "\n",
    "    new_test_data = new_data_t[1500:,:]\n",
    "    new_test_label = Y_test.argmax(axis=1)[1500:]\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)[1500:]==new_test_label)))\n",
    "\n",
    "if mode == 2:\n",
    "    new_train_data = new_data_v\n",
    "    new_train_label = Y_val.argmax(axis=1)\n",
    "\n",
    "    new_test_data = new_data_t\n",
    "    new_test_label = Y_test.argmax(axis=1)\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==new_test_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqqVxa0y8SWl"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf1 = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "      min_samples_split=2, random_state=0)\n",
    "clf1.fit(new_train_data, new_train_label)\n",
    "y_pred = clf1.predict(new_test_data)\n",
    "print(\"RandomForestClassifier \")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "# print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf2 = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "      min_samples_split=2, random_state=0)\n",
    "clf2.fit(new_train_data, new_train_label)\n",
    "y_pred = clf2.predict(new_test_data)\n",
    "print(\"ExtraTreesClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "# print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "clf3 = svm.SVC()\n",
    "clf3.fit(new_train_data, new_train_label)\n",
    "y_pred = clf3.predict(new_test_data)\n",
    "print(\"SVMClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "# print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf4 = MLPClassifier(random_state=1, max_iter=300).fit(new_train_data, new_train_label)\n",
    "y_pred = clf4.predict(new_test_data)\n",
    "print(\"MLPClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "# print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q0wCDX_0jxcZ"
   },
   "source": [
    "#Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1599049747123,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "BeNO_t18j1Y8",
    "outputId": "4e33dcc8-7ce8-4166-fc41-a639f3f6aff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_emb_new_sentence.shape=> (1, 768)\n",
      "1:new_sentence=> ['says the annies list political group supports thirdtrimester abortions on demand']\n",
      "2:new_sentence=> [[7, 1, 6738, 1046, 481, 566, 352, 4897, 529, 11, 1479]]\n",
      "3:new_sentence=> (1, 100)\n",
      "_credit_rank.shape=> (1,)\n",
      "******************\n",
      "******************\n",
      "Classificatin Results:\n",
      "Class: false => probability: 0.18773924\n",
      "Class: pants-fire => probability: 0.08303636\n",
      "Class: half-true => probability: 0.2002887\n",
      "Class: mostly-true => probability: 0.199299\n",
      "Class: barely-true => probability: 0.16621561\n",
      "Class: true => probability: 0.16342108\n",
      "******************\n",
      "******************\n",
      "Final Result for statement==>> says the annies list political group supports thirdtrimester abortions on demand\n",
      "Class: half-true => probability: 0.2002887\n",
      "Result of second stage of Classification:\n",
      "Predicted Label: half-true\n"
     ]
    }
   ],
   "source": [
    "new_sentence_ori = ['Says the Annies List political group supports third-trimester abortions on demand.']\n",
    "_emb_new_sentence = sentence_embedding(new_sentence_ori[0]).reshape(1, -1)\n",
    "print(\"_emb_new_sentence.shape=>\",_emb_new_sentence.shape)\n",
    "\n",
    "new_sentence = norm_all_sent(new_sentence_ori)\n",
    "print(\"1:new_sentence=>\",new_sentence)\n",
    "new_sentence = tokenizer.texts_to_sequences(new_sentence)\n",
    "print(\"2:new_sentence=>\",new_sentence)\n",
    "new_sentence = pad_sequences(new_sentence , maxlen = max_seq_len)\n",
    "print(\"3:new_sentence=>\",new_sentence.shape)\n",
    "\n",
    "\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "_dist = euclidean_distances(_emb_new_sentence,all_emb)\n",
    "_nearest_index = _dist.argmin()\n",
    "_credit_rank = cluster_weight[train_c[_nearest_index]]\n",
    "_credit_rank = _credit_rank.reshape(1)\n",
    "print(\"_credit_rank.shape=>\",_credit_rank.shape)\n",
    "\n",
    "y_pred_new_sentence = model.predict([new_sentence,_credit_rank])\n",
    "class_new_sentence = y_pred_new_sentence.argmax()\n",
    "label_dict = ['false','pants-fire','half-true','mostly-true','barely-true','true']\n",
    "\n",
    "print(\"******************\")\n",
    "print(\"******************\")\n",
    "print(\"Classificatin Results:\")\n",
    "for _i in range(len(label_dict)):\n",
    "    print(\"Class:\",label_dict[_i],\"=> probability:\",y_pred_new_sentence[0][_i])\n",
    "print(\"******************\")\n",
    "print(\"******************\")\n",
    "print(\"Final Result for statement==>>\", new_sentence_ori[0])\n",
    "print(\"Class:\",label_dict[class_new_sentence],\"=> probability:\",y_pred_new_sentence[0][class_new_sentence])\n",
    "\n",
    "new_data_for_next_step = np.array([y_pred_new_sentence[0][class_new_sentence],_credit_rank[0]])\n",
    "new_data_for_next_step = new_data_for_next_step.reshape(1, -1)\n",
    "\n",
    "\n",
    "y_pred = clf3.predict(new_data_for_next_step)\n",
    "print(\"Result of second stage of Classification:\")\n",
    "print(\"Predicted Label:\",label_dict[y_pred[0]])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPMDKRtopZP7wjCoeVNwSle",
   "collapsed_sections": [],
   "mount_file_id": "1VEpVfCKAY_ej8Avr4rFHrPUlUmsXJXqL",
   "name": "New1_Copy_of_Text6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxJ9oy_u5cxF"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "data_path = \"drive/My Drive/_data/\"\n",
    "!ls \"drive/My Drive/\"\n",
    "!ls \"drive/My Drive/_data\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('drive/My Drive/')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *#Embedding,Bidirectional,Dense,Conv1D,Flatten,LSTM,GlobalMaxPooling1D,Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1673,
     "status": "ok",
     "timestamp": 1599070172733,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "PL7PPhYD8NRB",
    "outputId": "2d3d3fee-687d-4546-ff8e-3d0ec1b66224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10238\n",
      "7\n",
      "Index(['Label', 'Statement', 'F1', 'F2', 'F3', 'F4', 'F5'], dtype='object')\n",
      "['false' 'half-true' 'mostly-true' 'true' 'barely-true' 'pants-fire']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Statement</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label                                          Statement  ...     F4    F5\n",
       "0        false  Says the Annies List political group supports ...  ...    0.0   0.0\n",
       "1    half-true  When did the decline of coal start? It started...  ...    1.0   0.0\n",
       "2  mostly-true  Hillary Clinton agrees with John McCain \"by vo...  ...  163.0   9.0\n",
       "3        false  Health care reform legislation is likely to ma...  ...    5.0  44.0\n",
       "4    half-true  The economic turnaround started at the end of ...  ...   19.0   2.0\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = data_path + \"valid.tsv\"\n",
    "df_v = pd.read_csv(data_file, header=None, sep='\\t', \n",
    "                 usecols=[1,2,8,9,10,11,12], \n",
    "                 names=['Label', 'Statement','F1','F2','F3','F4','F5'])\n",
    "df_v = df_v.dropna() \n",
    "\n",
    "data_file = data_path + \"test.tsv\"\n",
    "df_t = pd.read_csv(data_file, header=None, sep='\\t', \n",
    "                 usecols=[1,2,8,9,10,11,12], \n",
    "                 names=['Label', 'Statement','F1','F2','F3','F4','F5'])\n",
    "df_t = df_t.dropna()\n",
    "\n",
    "data_file = data_path + \"train.tsv\"\n",
    "df = pd.read_csv(data_file, header=None, sep='\\t', \n",
    "                 usecols=[1,2,8,9,10,11,12], \n",
    "                 names=['Label', 'Statement','F1','F2','F3','F4','F5'])\n",
    "df = df.dropna()\n",
    "\n",
    "print(len(df))\n",
    "print(len(df.columns))\n",
    "print(df.columns)\n",
    "print(df.Label.unique())\n",
    "df.head()\n",
    "# df['Statement'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA_GjGadT_7L"
   },
   "source": [
    "#preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hB4MvVAmULEn"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation_number(words):\n",
    "    \"\"\"Remove punctuation & number from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'([^\\s\\w]|_|[0-9])+', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation_number(words)\n",
    "    # words = remove_stopwords(words)\n",
    "    # words = stem_words(words)\n",
    "    # words = lemmatize_verbs(words)\n",
    "    return words\n",
    "\n",
    "def norm_all_sent(_sentences):\n",
    "  for _i,_w in enumerate(_sentences):\n",
    "    # print(_i,_w)\n",
    "    tmp_sentences = _w#_sentences[_w]\n",
    "    tmp_sentences = word_tokenize(tmp_sentences)\n",
    "    tmp_sentences = normalize(tmp_sentences)\n",
    "    tmp_sentences = \" \".join(tmp_sentences)\n",
    "    _sentences[_i] = tmp_sentences\n",
    "  return _sentences\n",
    "\n",
    "def convert_labels(_label):\n",
    "#['false' 'half-true' 'mostly-true' 'true' 'barely-true' 'pants-fire']  \n",
    "    _label[_label=='false']=0\n",
    "    _label[_label=='pants-fire']=0\n",
    "    _label[_label=='half-true']=1\n",
    "    _label[_label=='mostly-true']=1\n",
    "    _label[_label=='barely-true']=0\n",
    "    _label[_label=='true']=1\n",
    "    return _label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 3471,
     "status": "ok",
     "timestamp": 1599070584257,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "1OzG8TymqjA3",
    "outputId": "7cde3980-b03c-4ab6-c8c7-43858a6aa9a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(index_of_words) 12328\n"
     ]
    }
   ],
   "source": [
    "embed_num_dims = 100\n",
    "max_seq_len = 100\n",
    "\n",
    "sentences = df['Statement'].values\n",
    "sentences = norm_all_sent(sentences)\n",
    "Y = df['Label'].values\n",
    "Y = convert_labels(Y).astype(np.int)\n",
    "\n",
    "\n",
    "sentences_t = df_t['Statement'].values\n",
    "sentences_t = norm_all_sent(sentences_t)\n",
    "Y_t = df_t['Label'].values\n",
    "Y_t = convert_labels(Y_t).astype(np.int)\n",
    "\n",
    "\n",
    "sentences_v = df_v['Statement'].values\n",
    "sentences_v = norm_all_sent(sentences_v)\n",
    "Y_v = df_v['Label'].values\n",
    "Y_v = convert_labels(Y_v).astype(np.int)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)\n",
    "sequence_t = tokenizer.texts_to_sequences(sentences_t)\n",
    "sequence_v = tokenizer.texts_to_sequences(sentences_v)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"len(index_of_words)\",len(index_of_words))\n",
    "\n",
    "padded_seq = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "padded_seq_t = pad_sequences(sequence_t , maxlen = max_seq_len )\n",
    "padded_seq_v = pad_sequences(sequence_v , maxlen = max_seq_len )\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "Y = to_categorical(Y)\n",
    "Y_t = to_categorical(Y_t)\n",
    "Y_v = to_categorical(Y_v)\n",
    "\n",
    "X_train=padded_seq\n",
    "Y_train=Y\n",
    "X_test=padded_seq_t\n",
    "Y_test=Y_t\n",
    "X_val=padded_seq_v\n",
    "Y_val=Y_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpxFlNvWJOOz"
   },
   "source": [
    "#CredRank Algorithm\n",
    "##Measuring User Credibility in Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "executionInfo": {
     "elapsed": 1090,
     "status": "ok",
     "timestamp": 1599071027538,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "RAt6h1ZdoCQW",
    "outputId": "1511bbed-f56b-4e70-d771-7bede6523d0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_emb_train.shape=> (10238, 5)\n",
      "all_emb_test.shape=> (1267, 5)\n",
      "all_emb_valid.shape=> (1284, 5)\n",
      "tmp_all_emb.shape=> (12789, 5)\n"
     ]
    }
   ],
   "source": [
    "all_emb = df.iloc[:,2:].values\n",
    "print(\"all_emb_train.shape=>\",all_emb.shape)\n",
    "\n",
    "all_emb_t = df_t.iloc[:,2:].values\n",
    "print(\"all_emb_test.shape=>\",all_emb_t.shape)\n",
    "\n",
    "all_emb_v = df_v.iloc[:,2:].values\n",
    "print(\"all_emb_valid.shape=>\",all_emb_v.shape)\n",
    "\n",
    "\n",
    "tmp_all_emb = np.concatenate((all_emb,all_emb_t,all_emb_v))\n",
    "print(\"tmp_all_emb.shape=>\",tmp_all_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "executionInfo": {
     "elapsed": 4288,
     "status": "ok",
     "timestamp": 1599072834854,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "3ieYJ4WQdpUT",
    "outputId": "d14c6a1c-20a1-4c19-e9df-77f4dec7a1bd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAGeCAYAAABW9k9bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVO0lEQVR4nO3df7DldX3f8dc7LFaBEkC2LAGbpQ2a2Nhc4461dZLshKSi+QGtjoNYsjpktqPBxpi00k46tiadxJlMND+MnZ2gbKprQy0N1Dp2HHQnNTOhXfRWoviDYtGlu8uaIgIqSvLpH+fg7OAi9p733e+9Zx+PmTv3nO855573cfHe5/l+v+f7rTFGAABY3HdMPQAAwLIQVgAATYQVAEATYQUA0ERYAQA02TL1AEly7rnnju3bt089BgDAE7rtttu+MMbYerzbNkRYbd++PQcOHJh6DACAJ1RVdz/ebTYFAgA0EVYAAE2EFQBAE2EFANBEWAEANBFWAABNhBUAQBNhBQDQRFgBADQRVgAATYQVAEATYQUA0ERYAQA0EVYAAE2EFQBAky1TD3Ci7dmT7Ns39RRw4l15ZbJ799RTACy3k26N1b59yerq1FPAibW66g0FwIlw0q2xSpKVlWT//qmngBNn586pJwA4OTzhGquqentV3VtVf3bMsnOq6gNV9Zn597Pny6uqfruq7qyqj1XVD67n8AAAG8m3synw+iSXPmbZtUluGWNcnOSW+fUkeWGSi+dfu5O8rWdMAICN7wnDaozxx0n+72MWX5Zk7/zy3iSXH7P8D8bMnyY5q6rO7xoWAGAjW+vO6+eNMQ7NLx9Oct788gVJPn/M/Q7Ol32TqtpdVQeq6sDRo0fXOAYAwMax8KcCxxgjyVjD4/aMMXaMMXZs3bp10TEAACa31rA68ugmvvn3e+fL70nytGPud+F8GQDA0ltrWN2cZNf88q4kNx2z/Gfmnw58XpL7j9lkCACw1J7wOFZV9e4kO5OcW1UHk7whya8nuaGqrk5yd5KXzu/+viQvSnJnki8neeU6zAwAsCE9YViNMV72ODddcpz7jiQ/t+hQAACb0Ul3ShsAgPUirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmC4VVVf1CVX28qv6sqt5dVU+uqouq6taqurOq/rCqntQ1LADARrbmsKqqC5L8kyQ7xhjfn+SUJFckeVOSN48xvifJfUmu7hgUAGCjW3RT4JYkT6mqLUlOS3IoyY8mec/89r1JLl/wOQAANoU1h9UY454kv5Hkc5kF1f1JbkvyxTHGI/O7HUxywfEeX1W7q+pAVR04evToWscAANgwFtkUeHaSy5JclOS7kpye5NJv9/FjjD1jjB1jjB1bt25d6xgAABvGIpsCfyzJZ8cYR8cYX09yY5LnJzlrvmkwSS5Mcs+CMwIAbAqLhNXnkjyvqk6rqkpySZJPJPlQkpfM77MryU2LjQgAsDksso/VrZntpP6RJLfPf9aeJK9P8rqqujPJU5Nc1zAnAMCGt+WJ7/L4xhhvSPKGxyy+K8lzF/m5AACbkSOvAwA0EVYAAE2EFQBAE2EFANBEWAEANBFWAABNhBUAQBNhBQDQRFgBADQRVgAATYQVAEATYQUA0ERYAQA0EVYAAE2EFQBAE2EFANBEWAEANBFWAABNhBUAQBNhBQDQRFgBADQRVgAATYQVAEATYQUA0ERYAQA0EVYAAE2EFQBAE2EFANBEWAEANBFWAABNhBUAQBNhBQDQRFgBADQRVgAATYQVAEATYQUA0ERYAQA0EVYAAE2EFQBAE2EFANBEWAEANBFWAABNhBUAQBNhBQDQRFgBADQRVgAATYQVAEATYQUA0ERYAQA0EVYAAE2EFQBAE2EFANBEWAEANBFWAABNhBUAQBNhBQDQRFgBADQRVgAATYQVAEATYQUA0ERYAQA0EVYAAE0WCquqOquq3lNVn6yqO6rq71bVOVX1gar6zPz72V3DAgBsZIuusfqtJO8fY3xvkh9IckeSa5PcMsa4OMkt8+sAAEtvzWFVVd+Z5IeTXJckY4yvjTG+mOSyJHvnd9ub5PJFhwQA2AwWWWN1UZKjSd5RVR+tqt+vqtOTnDfGODS/z+Ek5x3vwVW1u6oOVNWBo0ePLjAGAMDGsEhYbUnyg0neNsZ4dpKH8pjNfmOMkWQc78FjjD1jjB1jjB1bt25dYAwAgI1hkbA6mOTgGOPW+fX3ZBZaR6rq/CSZf793sREBADaHNYfVGONwks9X1TPmiy5J8okkNyfZNV+2K8lNC00IALBJbFnw8a9J8q6qelKSu5K8MrNYu6Gqrk5yd5KXLvgcAACbwkJhNcZYTbLjODddssjPBQDYjBx5HQCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmmyZegB67bltT/bdvm/qMdhgVg+/JUmy8/rXTjwJG82Vz7oyu5+ze+oxYGkIqyWz7/Z9WT28mpVtK1OPwgaycq2g4putHl5NEmEFjYTVElrZtpL9r9g/9RjABrfz+p1TjwBLxz5WAABNhBUAQBNhBQDQRFgBADQRVgAATYQVAEATYQUA0ERYAQA0EVYAAE0WDquqOqWqPlpV751fv6iqbq2qO6vqD6vqSYuPCQCw8XWssfr5JHccc/1NSd48xvieJPclubrhOQAANryFwqqqLkzyE0l+f369kvxokvfM77I3yeWLPAcAwGax6BqrtyT5Z0n+cn79qUm+OMZ4ZH79YJILjvfAqtpdVQeq6sDRo0cXHAMAYHprDquq+skk944xblvL48cYe8YYO8YYO7Zu3brWMQAANowtCzz2+Ul+uqpelOTJSc5M8ltJzqqqLfO1VhcmuWfxMQEANr41r7EaY/zzMcaFY4ztSa5I8sExxsuTfCjJS+Z325XkpoWnBADYBNbjOFavT/K6qrozs32urluH5wAA2HAW2RT4DWOM/Un2zy/fleS5HT8XAGAzceR1AIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoMmaw6qqnlZVH6qqT1TVx6vq5+fLz6mqD1TVZ+bfz+4bFwBg41pkjdUjSX5xjPHMJM9L8nNV9cwk1ya5ZYxxcZJb5tcBAJbemsNqjHFojPGR+eUHktyR5IIklyXZO7/b3iSXLzokAMBm0LKPVVVtT/LsJLcmOW+McWh+0+Ek5z3OY3ZX1YGqOnD06NGOMQAAJrVwWFXVGUn+Y5LXjjG+dOxtY4yRZBzvcWOMPWOMHWOMHVu3bl10DACAyS0UVlV1amZR9a4xxo3zxUeq6vz57ecnuXexEQEANodFPhVYSa5LcscY4zePuenmJLvml3cluWnt4wEAbB5bFnjs85NcleT2qlqdL/sXSX49yQ1VdXWSu5O8dLERAQA2hzWH1Rjjw0nqcW6+ZK0/FwBgs3LkdQCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCZbph4AeGJ7btuTfbfvm3oMlszq4dUkyc7rd047COvq0AOHcuShIyf0OVe2rZzQ57vyWVdm93N2n9DnfDzWWMEmsO/2fd/4IwhdVratnPA/gJx4Rx46kge/9uDUY6yb1cOrG+qNpzVWsEmsbFvJ/lfsn3oMYJN5dI3ksv7+2GhrXK2xAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmzhXIprLntj0b6mSbJ8qjJ2DeaOfEOlE20pnrAb4Va6zYVPbdvu8bkXEyWdm2kpVtK1OPMYmNduZ6gG/FGis2nZVtK0t7lna+2cm6lg7YnKyxAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmggrAIAmwgoAoImwAgBoIqwAAJoIKwCAJsIKAKCJsAIAaCKsAACaCCsAgCbCCgCgibACAGgirAAAmmyZegBgZs9te7Lv9n1Tj7HhrB5eTZLsvH7ntINsQFc+68rsfs7uqccAjiGsYIPYd/u+rB5ezcq2lalH2VDW+r/HoQcO5chDR5qn2Tjuf/j+rB5eXeoYF45sRsIKNpCVbSvZ/4r9U4+xFHZevzNHHjoiVJuc6FCdIhyFHB2EFbC0hGqfZQ/VRzc5CysWJawA+LYsc6jah48uPhUIANBEWAEANFmXsKqqS6vqU1V1Z1Vdux7PAQCw0bSHVVWdkuStSV6Y5JlJXlZVz+x+HgCAjWY9dl5/bpI7xxh3JUlV/fsklyX5xDo8F7CJnMgdhKc4sOiy7tgNfPtqjNH7A6tekuTSMcbPzq9fleTvjDGuecz9did59HOtz0jyqdZBAADWx3ePMbYe74bJDrcwxtiTZM9Uzw8A0G09dl6/J8nTjrl+4XwZAMBSW4+w+h9JLq6qi6rqSUmuSHLzOjwPAMCG0r4pcIzxSFVdk+S/JjklydvHGB/vfh4AgI2mfed1AICTlSOvAwA0EVYAAE2EFQBAk5MmrKrqr1TVdVV1d1U9UFWrVfXCqefqVFXnVNV/qqqH5q/zyqln6lZVF1fVV6vqnVPP0qmq3llVh6rqS1X16ar62aln6lZVV1TVHfP/Pv9XVf3Q1DN1qartVfW+qrqvqg5X1e9W1WTHCexUVddU1YGqeriqrp96nvVQVfvnv1cenH8tzQGrj3lNj379RVX9ztRzdamq76uqD1bV/fPzE/+DqWc6acIqs09Afj7JjyT5ziS/nOSGqto+4Uzd3prka0nOS/LyJG+rqr817Ujt3prZIT2Wza8l2T7GODPJTyf51ap6zsQztamqH0/ypiSvTPJXk/xwkrsmHarX7yW5N8n5SVYy+z3z6kkn6vN/kvxqkrdPPcg6u2aMccb86xlTD9PlmNd0RpJtSb6S5D9MPFaL+ZuXm5K8N8k5mZ3N5Z1V9fQp5zppwmqM8dAY41+NMf73GOMvxxjvTfLZJEvxx6uqTk/y4iT/cozx4Bjjw5kdP+yqaSfrU1VXJPliklumnqXbGOPjY4yHH706//qbE47U7V8neeMY40/n//+7Z4yxTAcOvijJDWOMr44xDid5f5KleFMzxrhxjPFHSf586llY2IszewPw36YepMn3JvmuJG8eY/zFGOODSf4kE//dO2nC6rGq6rwkT0+yLMfYenqSR8YYnz5m2f/Mkvxyr6ozk7wxyeumnmW9VNXvVdWXk3wyyaEk75t4pBZVdUqSHUm2zlfVH5xvKnvK1LM1ekuSK6rqtKq6IMkLM4srNo9fq6ovVNWfVNXOqYdZJ7uS/MFY7uMsVZLvn3KAkzKsqurUJO9KsneM8cmp52lyRpIvPWbZ/ZltdlkGv5LkujHGwakHWS9jjFdn9u/1Q0luTPLwt37EpnFeklOTvCSz17aS5NmZbY5fFn+c2ZuYLyU5mORAkj+adCL+f7w+yd9IckFm57D9z1W1TGuMU1Xfndkm6r1Tz9LoU5mtgfunVXVqVf39zF7jaVMOddKFVVV9R5J/l9m+SNdMPE6nB5Oc+ZhlZyZ5YIJZWlXVSpIfS/LmqWdZb/PV2R/O7Bybr5p6niZfmX//nTHGoTHGF5L8ZpIXTThTm/nvlPdnFsOnJzk3ydmZ7VPGJjDGuHWM8cAY4+Exxt7MNictxX+fx7gqyYfHGJ+depAuY4yvJ7k8yU8kOZzkF5PckNmbm8mcVGFVVZXkuszeQb94/o+yLD6dZEtVXXzMsh/Icmzq3Jlke5LPVdXhJL+U5MVV9ZEph1pnW7Ik+1iNMe7L7BfdsZsflmlTxDlJ/nqS353/Yf7zJO/I8v1hPpmMzDYpLZOfyXKtrUqSjDE+Nsb4kTHGU8cYL8hszeN/n3Kmkyqskrwtyfcl+akxxlee6M6byRjjoczeMb+xqk6vqucnuSyztXOb3Z7MImNl/vVvk/yXJC+YcqguVfXX5ociOKOqTqmqFyR5WZZrJ/13JHnN/LWeneQXMvskz6Y3XwP32SSvqqotVXVWZvuyfGzayXrMX9OTMzv36ylV9eRlOZREklTVWVX1gkdfV1W9PLNPrS7NPnJV9fcy28y5FJ8GPFZV/e35v91pVfVLmX0y9/opZzppwmq+ffkfZ/aH+fAxx/R4+cSjdXp1kqdkts353UletQwnwB5jfHmMcfjRr8w2e351jHF06tmajMw2+x1Mcl+S30jy2jHGzZNO1etXMjtMxqeT3JHko0n+zaQT9fqHSS5NcjTJnUm+nlk8LoNfzmxz7rVJ/tH88jLtH3dqZoeTOJrkC0lek+Tyx3wQaLPbleTGMcam3zXkOK7K7MM+9ya5JMmPH/MJ60k4CTMAQJOTZo0VAMB6E1YAAE2EFQBAE2EFANBEWAEANBFWAABNhBUAQBNhBQDQ5P8BgUAn6SJkGDoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "linked = linkage(all_emb_t[:10,:], 'single')\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram (linked,\n",
    "            orientation='top',\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster = AgglomerativeClustering(n_clusters=100,affinity='euclidean', linkage='ward')# affinity=\"cosine\", linkage='average')\n",
    "tmp_c=cluster.fit_predict(tmp_all_emb)\n",
    "\n",
    "# outfile = data_path + \"clustering_result_new.npy\"\n",
    "# np.save(outfile, tmp_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1599072842566,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "MMVp7AiLDYbs",
    "outputId": "63f5c343-5e84-4346-ee00-563dae846116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10238\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Counter({0: 713, 14: 681, 12: 556, 71: 488, 3: 400, 96: 368, 90: 337, 24: 289, 64: 273, 2: 257, 25: 239, 20: 221, 18: 187, 44: 176, 50: 158, 9: 158, 54: 149, 27: 148, 39: 142, 29: 142, 49: 128, 35: 122, 83: 117, 56: 115, 37: 103, 1: 102, 5: 98, 95: 93, 55: 88, 46: 82, 15: 79, 11: 78, 59: 78, 40: 78, 10: 77, 47: 72, 26: 71, 36: 70, 41: 70, 16: 69, 6: 69, 93: 69, 22: 68, 84: 65, 91: 63, 57: 63, 8: 61, 33: 59, 4: 57, 61: 56, 42: 56, 45: 55, 99: 52, 79: 49, 23: 49, 86: 49, 13: 48, 65: 46, 43: 46, 69: 46, 7: 44, 76: 44, 52: 43, 34: 43, 31: 42, 17: 42, 28: 42, 81: 39, 32: 38, 30: 37, 51: 37, 88: 37, 48: 37, 58: 36, 87: 35, 94: 35, 82: 34, 72: 34, 21: 32, 38: 32, 70: 31, 73: 31, 53: 30, 62: 29, 75: 29, 68: 29, 60: 27, 85: 26, 74: 25, 89: 24, 98: 24, 63: 23, 92: 23, 97: 21, 80: 20, 66: 19, 78: 19, 19: 19, 77: 14, 67: 14})\n",
      "1267\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "Counter({0: 79, 71: 62, 14: 61, 12: 57, 3: 47, 24: 46, 96: 45, 90: 35, 64: 33, 25: 31, 2: 26, 20: 25, 18: 24, 56: 21, 27: 21, 35: 20, 49: 20, 1: 19, 50: 19, 15: 18, 5: 18, 29: 17, 44: 17, 54: 17, 9: 16, 83: 14, 46: 14, 16: 14, 59: 13, 39: 13, 40: 12, 4: 12, 52: 12, 10: 12, 79: 11, 17: 11, 95: 11, 26: 11, 37: 11, 23: 10, 57: 10, 33: 10, 61: 9, 69: 8, 99: 8, 91: 8, 74: 8, 36: 8, 41: 8, 22: 7, 43: 7, 11: 7, 97: 7, 6: 7, 55: 7, 7: 7, 72: 7, 60: 6, 78: 6, 66: 6, 28: 6, 47: 6, 84: 6, 93: 6, 76: 6, 85: 5, 48: 5, 30: 5, 45: 5, 81: 5, 58: 5, 31: 5, 75: 4, 13: 4, 82: 4, 42: 4, 21: 4, 94: 4, 70: 4, 8: 4, 80: 3, 98: 3, 92: 3, 86: 3, 19: 3, 68: 3, 38: 3, 34: 3, 67: 2, 65: 2, 51: 2, 89: 2, 32: 2, 87: 2, 73: 2, 53: 2, 77: 2, 63: 1, 62: 1})\n",
      "1284\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "Counter({0: 86, 14: 72, 12: 70, 71: 61, 96: 53, 3: 48, 2: 45, 24: 37, 64: 37, 90: 31, 25: 27, 18: 27, 20: 24, 39: 23, 83: 22, 27: 20, 44: 19, 54: 17, 9: 16, 56: 14, 29: 14, 95: 14, 6: 13, 41: 13, 59: 13, 1: 13, 49: 13, 50: 13, 52: 12, 45: 12, 36: 12, 33: 12, 23: 11, 55: 11, 46: 11, 40: 10, 15: 10, 4: 10, 26: 10, 5: 10, 42: 9, 82: 9, 75: 8, 37: 8, 79: 8, 35: 8, 81: 8, 73: 8, 43: 8, 93: 8, 47: 8, 17: 7, 98: 7, 10: 7, 84: 7, 51: 7, 32: 7, 58: 6, 57: 6, 94: 6, 30: 6, 31: 6, 8: 6, 60: 6, 34: 6, 99: 6, 28: 6, 91: 5, 21: 5, 63: 5, 11: 5, 69: 5, 22: 5, 61: 5, 16: 5, 70: 5, 76: 4, 13: 4, 86: 4, 48: 4, 65: 4, 19: 3, 7: 3, 62: 3, 97: 3, 89: 3, 85: 3, 78: 3, 74: 2, 66: 2, 88: 2, 77: 2, 38: 2, 53: 2, 72: 2, 68: 2, 67: 2, 92: 1, 80: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# data_file = data_path + \"clustering_result_new.npy\"\n",
    "# tmp_c = np.load(data_file)\n",
    "\n",
    "\n",
    "train_c = tmp_c[:len(all_emb)]\n",
    "print(len(train_c))\n",
    "print(np.unique(train_c))\n",
    "print(Counter(train_c))\n",
    "# print(Counter(Y_train.argmax(axis=1)))\n",
    "\n",
    "test_c = tmp_c[len(all_emb):len(all_emb)+len(all_emb_t)]\n",
    "print(len(test_c))\n",
    "print(np.unique(test_c))\n",
    "print(Counter(test_c))\n",
    "# print(Counter(Y_train.argmax(axis=1)))\n",
    "\n",
    "valid_c = tmp_c[len(all_emb)+len(all_emb_t):]\n",
    "print(len(valid_c))\n",
    "print(np.unique(valid_c))\n",
    "print(Counter(valid_c))\n",
    "\n",
    "\n",
    "n_of_m = Counter(train_c)\n",
    "cluster_weight = np.zeros(len(n_of_m))\n",
    "_tmp = np.array(list(n_of_m.values()))\n",
    "_for_div = np.sum(np.sqrt(_tmp))\n",
    "for _cluster in n_of_m:\n",
    "    _c_i = np.sqrt(n_of_m[_cluster])\n",
    "    # member_weight = np.sqrt(n_of_m[_cluster])/n_of_m[_cluster]\n",
    "    cluster_weight[_cluster] = (_c_i/_for_div)#*member_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYUTV5m3LV0B"
   },
   "source": [
    "#Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 13355,
     "status": "ok",
     "timestamp": 1599071152448,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "Aq0kBzWtufEg",
    "outputId": "3613d388-81f6-44c1-a35a-1499ffb1920a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove*.zip\n",
    "# !mv glove*.100d.txt 'drive/My Drive/_MrAnsari/_data'\n",
    "\n",
    "f = open(data_path+'glove.6B.100d.txt')\n",
    "embedd_index = {}\n",
    "for line in f:\n",
    "    val = line.split()\n",
    "    word = val[0]\n",
    "    coff = np.asarray(val[1:],dtype = 'float')\n",
    "    embedd_index[word] = coff\n",
    "\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embedd_index))\n",
    "\n",
    "# a_file = open(data_path+\"data.pkl\", \"wb\")\n",
    "# pickle.dump(embedd_index, a_file)\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-LYjN4NvkkU"
   },
   "outputs": [],
   "source": [
    "# a_file = open(data_path+\"data.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print('Found %s word vectors.' % len(embedd_index))\n",
    "\n",
    "embedd_index['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1599071164532,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "se70c_6sP_VP",
    "outputId": "2099d835-1158-45fa-8174-692e619d61d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12329, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(index_of_words) + 1, embed_num_dims))\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "\n",
    "for word,i in index_of_words.items():\n",
    "    temp = embedd_index.get(word)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp\n",
    "        \n",
    "#for plotting\n",
    "        tokens.append(embedding_matrix[i])\n",
    "        labels.append(word)\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9w3uUkwQlDT"
   },
   "outputs": [],
   "source": [
    "#TSNE algorithm used to visualize word embeddings having huge amount (100) dimensions\n",
    "\n",
    "def tsne():\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens[:200])\n",
    "    print(new_values.shape)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16,16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "tsne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 6724,
     "status": "ok",
     "timestamp": 1599071238918,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "Sj5nPst4Q_3G",
    "outputId": "4688a186-8f85-4e93-f525-7eb75098fb54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 100)      1232900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 50, 128)      84480       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           4128        global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            66          dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3)            0           dense_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            8           concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,321,582\n",
      "Trainable params: 1,321,582\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "input1 = Input(shape=(max_seq_len,))\n",
    "input2 = Input(shape=(1,))\n",
    "\n",
    "#Embedding layer before the actaul BLSTM \n",
    "embedd_layer = Embedding(len(index_of_words) + 1 , \n",
    "                         embed_num_dims , \n",
    "                         input_length = max_seq_len , \n",
    "                         weights = [embedding_matrix])(input1)\n",
    "BiLSTM = Bidirectional(LSTM(64 , return_sequences = True , dropout = 0.2 , recurrent_dropout = 0.2))(embedd_layer)\n",
    "GMP = GlobalMaxPooling1D()(BiLSTM)\n",
    "FC1 = Dense(32,activation = 'relu')(GMP)\n",
    "FC1_d = Dropout(0.2)(FC1)\n",
    "FC2 = Dense(2,activation = 'sigmoid')(FC1_d)\n",
    "_newinput = Concatenate()([FC2, input2])\n",
    "FC3 = Dense(2,activation = 'sigmoid')(_newinput)\n",
    "model = Model(inputs=[input1, input2], outputs=FC3)\n",
    "model.summary()\n",
    "\n",
    "# _adam = Adam(lr = 0.01)\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = 'rmsprop' , metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mz9tgyJvwQ9a"
   },
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for _i in range(len(train_c)):\n",
    "    tmp_data = cluster_weight[train_c[_i]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(len(test_c)):\n",
    "    tmp_data = cluster_weight[test_c[_i]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(len(valid_c)):\n",
    "    tmp_data = cluster_weight[valid_c[_i]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "executionInfo": {
     "elapsed": 151775,
     "status": "ok",
     "timestamp": 1599071421920,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "zlPOTCytRXp5",
    "outputId": "d211f430-a363-4892-ecb6-dd15fa78eacc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 14s 359ms/step - loss: 0.6800 - accuracy: 0.5618 - val_loss: 0.6781 - val_accuracy: 0.5202\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 14s 354ms/step - loss: 0.6681 - accuracy: 0.5651 - val_loss: 0.6734 - val_accuracy: 0.5475\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 14s 351ms/step - loss: 0.6628 - accuracy: 0.5927 - val_loss: 0.6917 - val_accuracy: 0.5382\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 14s 345ms/step - loss: 0.6587 - accuracy: 0.6161 - val_loss: 0.6706 - val_accuracy: 0.6160\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 14s 351ms/step - loss: 0.6553 - accuracy: 0.6241 - val_loss: 0.6705 - val_accuracy: 0.6012\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 0.6502 - accuracy: 0.6432 - val_loss: 0.6682 - val_accuracy: 0.6223\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 14s 350ms/step - loss: 0.6436 - accuracy: 0.6532 - val_loss: 0.6665 - val_accuracy: 0.6223\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 15s 363ms/step - loss: 0.6400 - accuracy: 0.6613 - val_loss: 0.6656 - val_accuracy: 0.6269\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 0.6338 - accuracy: 0.6688 - val_loss: 0.6595 - val_accuracy: 0.6363\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 14s 349ms/step - loss: 0.6268 - accuracy: 0.6802 - val_loss: 0.6573 - val_accuracy: 0.6363\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 0.6648 - accuracy: 0.5912\n",
      "59.116023778915405\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([X_train,new_data],Y_train,epochs = 10 , batch_size = 256, validation_data = ([X_val,new_data_v],Y_val))\n",
    "\n",
    "result = model.evaluate([X_test,new_data_t],Y_test)\n",
    "print(result[1]*100)\n",
    "\n",
    "model_file = data_path + \"LSTM_Model6_new_emb.h5\"\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 2177,
     "status": "ok",
     "timestamp": 1599071424126,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "1qaL4gamT4vl",
    "outputId": "916b4d9a-1d5b-407e-d48f-4557051c03ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 24ms/step - loss: 0.6648 - accuracy: 0.5912\n",
      "59.116023778915405\n"
     ]
    }
   ],
   "source": [
    "model_file = data_path + \"LSTM_Model6_new_emb.h5\"\n",
    "model.load_weights(model_file)\n",
    "\n",
    "result = model.evaluate([X_test,new_data_t],Y_test)\n",
    "print(result[1]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 12093,
     "status": "ok",
     "timestamp": 1599071434061,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "T6rIGGkh1lyU",
    "outputId": "fad53114-5652-45a2-b672-7e037ce7d5c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Test_Accuracy=> 0.5911602209944752\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "new_data = []\n",
    "for _i in range(len(train_c)):\n",
    "    tmp_data = cluster_weight[train_c[_i]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(len(test_c)):\n",
    "    tmp_data = cluster_weight[test_c[_i]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(len(valid_c)):\n",
    "    tmp_data = cluster_weight[valid_c[_i]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "#######################################\n",
    "#######################################\n",
    "\n",
    "y_pred_v  = model.predict([X_val,new_data_v])\n",
    "# print(\"Valid_Accuracy=>\",np.mean((y_pred_v.argmax(axis=1)==Y_val.argmax(axis=1))))\n",
    "\n",
    "y_pred_t  = model.predict([X_test,new_data_t])\n",
    "# print(\"Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==Y_test.argmax(axis=1))))\n",
    "\n",
    "y_pred  = model.predict([X_train,new_data])\n",
    "# print(\"Train_Accuracy=>\",np.mean((y_pred.argmax(axis=1)==Y_train.argmax(axis=1))))\n",
    "\n",
    "new_data = []\n",
    "for _i in range(y_pred.shape[0]):\n",
    "    tmp_data = [cluster_weight[train_c[_i]],y_pred[_i,1]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(y_pred_t.shape[0]):\n",
    "    tmp_data = [cluster_weight[test_c[_i]],y_pred_t[_i,1]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(y_pred_v.shape[0]):\n",
    "    tmp_data = [cluster_weight[valid_c[_i]],y_pred_v[_i,1]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "\n",
    "\n",
    "mode = 2\n",
    "if mode == 0:\n",
    "    new_train_data = new_data\n",
    "    new_train_label = Y_train.argmax(axis=1)\n",
    "\n",
    "    new_test_data = new_data_t\n",
    "    new_test_label = Y_test.argmax(axis=1)\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==new_test_label)))\n",
    "\n",
    "if mode == 1:\n",
    "    new_train_data = new_data_t[:1500,:]\n",
    "    new_train_label = Y_test.argmax(axis=1)[:1500]\n",
    "\n",
    "    new_test_data = new_data_t[1500:,:]\n",
    "    new_test_label = Y_test.argmax(axis=1)[1500:]\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)[1500:]==new_test_label)))\n",
    "\n",
    "if mode == 2:\n",
    "    new_train_data = new_data_v\n",
    "    new_train_label = Y_val.argmax(axis=1)\n",
    "\n",
    "    new_test_data = new_data_t\n",
    "    new_test_label = Y_test.argmax(axis=1)\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==new_test_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 12739,
     "status": "ok",
     "timestamp": 1599071434716,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "aqqVxa0y8SWl",
    "outputId": "fd384504-cc82-4caa-9744-0408b25d5f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier \n",
      "Confusion matrix(test):\n",
      "[[316 237]\n",
      " [339 375]]\n",
      "Test_Accuracy=> 0.5453827940015785\n",
      "ExtraTreesClassifier\n",
      "Confusion matrix(test):\n",
      "[[312 241]\n",
      " [337 377]]\n",
      "Test_Accuracy=> 0.5438042620363063\n",
      "SVMClassifier\n",
      "Confusion matrix(test):\n",
      "[[315 238]\n",
      " [274 440]]\n",
      "Test_Accuracy=> 0.595895816890292\n",
      "MLPClassifier\n",
      "Confusion matrix(test):\n",
      "[[315 238]\n",
      " [274 440]]\n",
      "Test_Accuracy=> 0.595895816890292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf1 = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "      min_samples_split=2, random_state=0)\n",
    "clf1.fit(new_train_data, new_train_label)\n",
    "y_pred = clf1.predict(new_test_data)\n",
    "print(\"RandomForestClassifier \")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "# print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf2 = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "      min_samples_split=2, random_state=0)\n",
    "clf2.fit(new_train_data, new_train_label)\n",
    "y_pred = clf2.predict(new_test_data)\n",
    "print(\"ExtraTreesClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "# print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "clf3 = svm.SVC()\n",
    "clf3.fit(new_train_data, new_train_label)\n",
    "y_pred = clf3.predict(new_test_data)\n",
    "print(\"SVMClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "# print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf4 = MLPClassifier(random_state=1, max_iter=300).fit(new_train_data, new_train_label)\n",
    "y_pred = clf4.predict(new_test_data)\n",
    "print(\"MLPClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "# print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0wCDX_0jxcZ"
   },
   "source": [
    "#Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1599073025424,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "BeNO_t18j1Y8",
    "outputId": "7e9d15da-84d0-4a67-b6b0-07b1057790fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_emb_new_sentence.shape=> (1, 5)\n",
      "1:new_sentence=> ['says the annies list political group supports thirdtrimester abortions on demand']\n",
      "2:new_sentence=> [[7, 1, 6734, 1045, 480, 565, 352, 4896, 541, 11, 1476]]\n",
      "3:new_sentence=> (1, 50)\n",
      "_credit_rank.shape=> (1,)\n",
      "******************\n",
      "******************\n",
      "Classificatin Results:\n",
      "Class: false => probability: 0.5205762\n",
      "Class: true => probability: 0.4794238\n",
      "******************\n",
      "******************\n",
      "Final Result for statement==>> says the annies list political group supports thirdtrimester abortions on demand\n",
      "Class: false => probability: 0.5205762\n",
      "******************\n",
      "##################\n",
      "******************\n",
      "Result of second stage of Classification:\n",
      "Predicted Label: false\n"
     ]
    }
   ],
   "source": [
    "new_sentence_ori = ['Says the Annies List political group supports third-trimester abortions on demand.']\n",
    "_emb_new_sentence = [1,13,12,14,3]\n",
    "_emb_new_sentence = np.array(_emb_new_sentence).reshape(1, -1)\n",
    "print(\"_emb_new_sentence.shape=>\",_emb_new_sentence.shape)\n",
    "\n",
    "new_sentence = norm_all_sent(new_sentence_ori)\n",
    "print(\"1:new_sentence=>\",new_sentence)\n",
    "new_sentence = tokenizer.texts_to_sequences(new_sentence)\n",
    "print(\"2:new_sentence=>\",new_sentence)\n",
    "new_sentence = pad_sequences(new_sentence , maxlen = max_seq_len)\n",
    "print(\"3:new_sentence=>\",new_sentence.shape)\n",
    "\n",
    "\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "_dist = euclidean_distances(_emb_new_sentence,all_emb)\n",
    "_nearest_index = _dist.argmin()\n",
    "_credit_rank = cluster_weight[train_c[_nearest_index]]\n",
    "_credit_rank = _credit_rank.reshape(1)\n",
    "print(\"_credit_rank.shape=>\",_credit_rank.shape)\n",
    "\n",
    "y_pred_new_sentence = model.predict([new_sentence,_credit_rank])\n",
    "from scipy.special import softmax\n",
    "y_pred_new_sentence = softmax(y_pred_new_sentence)\n",
    "\n",
    "class_new_sentence = y_pred_new_sentence.argmax()\n",
    "label_dict = ['false','true']\n",
    "\n",
    "print(\"******************\")\n",
    "print(\"******************\")\n",
    "print(\"Classificatin Results:\")\n",
    "for _i in range(len(label_dict)):\n",
    "    print(\"Class:\",label_dict[_i],\"=> probability:\",y_pred_new_sentence[0][_i])\n",
    "print(\"******************\")\n",
    "print(\"******************\")\n",
    "print(\"Final Result for statement==>>\", new_sentence_ori[0])\n",
    "print(\"Class:\",label_dict[class_new_sentence],\"=> probability:\",y_pred_new_sentence[0][class_new_sentence])\n",
    "\n",
    "new_data_for_next_step = np.array([y_pred_new_sentence[0][class_new_sentence],_credit_rank[0]])\n",
    "new_data_for_next_step = new_data_for_next_step.reshape(1, -1)\n",
    "\n",
    "print(\"******************\")\n",
    "print(\"##################\")\n",
    "print(\"******************\")\n",
    "\n",
    "y_pred = clf4.predict(new_data_for_next_step)\n",
    "print(\"Result of second stage of Classification:\")\n",
    "print(\"Predicted Label:\",label_dict[y_pred[0]])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPQ05VbijsYZDVcX/KGyQl5",
   "collapsed_sections": [],
   "mount_file_id": "1VEpVfCKAY_ej8Avr4rFHrPUlUmsXJXqL",
   "name": "New2_Copy_of_Text6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

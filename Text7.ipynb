{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58919,
     "status": "ok",
     "timestamp": 1599044715604,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "FxJ9oy_u5cxF",
    "outputId": "f6db2355-7f26-4079-c20e-0dea3ee0e4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "classifier.py  FeatureSelection.py\tText0.ipynb  Text4.ipynb\n",
      "_config.yml    New_Copy_of_Text6.ipynb\tText1.ipynb  Text5.ipynb\n",
      "_data\t       prediction.py\t\tText2.ipynb  Text6.ipynb\n",
      "DataPrep.py    __pycache__\t\tText3.ipynb  Text7.ipynb\n",
      "clustering_result.npy\t  LSTM_Model4.h5      test.npy\t     train.tsv\n",
      "en_wiki_word2vec_300.zip  LSTM_Model6.h5      test.tsv\t     valid.csv\n",
      "glove.6B.100d.txt\t  LSTM_Model6_new.h5  train.csv      valid_new.npy\n",
      "glove.6B.50d.txt\t  test.csv\t      train_new.npy  valid.npy\n",
      "LSTM_Model3.h5\t\t  test_new.npy\t      train.npy      valid.tsv\n",
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "data_path = \"drive/My Drive/_data/\"\n",
    "!ls \"drive/My Drive/\"\n",
    "!ls \"drive/My Drive/_data\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('drive/My Drive/')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *#Embedding,Bidirectional,Dense,Conv1D,Flatten,LSTM,GlobalMaxPooling1D,Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1001,
     "status": "ok",
     "timestamp": 1599044826577,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "PL7PPhYD8NRB",
    "outputId": "03ffc031-2c90-4549-c68c-f814d2244712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240\n",
      "2\n",
      "Index(['Statement', 'Label'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statement  Label\n",
       "0  Says the Annies List political group supports ...  False\n",
       "1  When did the decline of coal start? It started...   True\n",
       "2  Hillary Clinton agrees with John McCain \"by vo...   True\n",
       "3  Health care reform legislation is likely to ma...  False\n",
       "4  The economic turnaround started at the end of ...   True"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = data_path + \"valid.csv\"\n",
    "df_v = pd.read_csv(data_file)\n",
    "df_v = df_v.dropna() \n",
    "\n",
    "data_file = data_path + \"test.csv\"\n",
    "df_t = pd.read_csv(data_file)\n",
    "df_t = df_t.dropna()\n",
    "\n",
    "data_file = data_path + \"train.csv\"\n",
    "df = pd.read_csv(data_file)\n",
    "df = df.dropna()\n",
    "\n",
    "print(len(df))\n",
    "print(len(df.columns))\n",
    "print(df.columns)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nA_GjGadT_7L"
   },
   "source": [
    "#preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 789,
     "status": "ok",
     "timestamp": 1599044984497,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "hB4MvVAmULEn"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation_number(words):\n",
    "    \"\"\"Remove punctuation & number from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'([^\\s\\w]|_|[0-9])+', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation_number(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = stem_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return words\n",
    "\n",
    "def norm_all_sent(_sentences):\n",
    "  for _i,_w in enumerate(_sentences):\n",
    "    # print(_i,_w)\n",
    "    tmp_sentences = _w#_sentences[_w]\n",
    "    tmp_sentences = word_tokenize(tmp_sentences)\n",
    "    tmp_sentences = normalize(tmp_sentences)\n",
    "    tmp_sentences = \" \".join(tmp_sentences)\n",
    "    _sentences[_i] = tmp_sentences\n",
    "  return _sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4503,
     "status": "ok",
     "timestamp": 1599044840262,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "1OzG8TymqjA3",
    "outputId": "901ae671-7e5b-47be-cdec-de5347762d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(index_of_words) 12327\n"
     ]
    }
   ],
   "source": [
    "embed_num_dims = 100\n",
    "max_seq_len = 50\n",
    "\n",
    "sentences = df['Statement'].values\n",
    "sentences = norm_all_sent(sentences)\n",
    "Y = df['Label'].values\n",
    "\n",
    "sentences_t = df_t['Statement'].values\n",
    "sentences_t = norm_all_sent(sentences_t)\n",
    "Y_t = df_t['Label'].values\n",
    "\n",
    "sentences_v = df_v['Statement'].values\n",
    "sentences_v = norm_all_sent(sentences_v)\n",
    "Y_v = df_v['Label'].values\n",
    "Y_v = np.where(Y_v=='TRUE',True,False)\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 4000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)\n",
    "sequence_t = tokenizer.texts_to_sequences(sentences_t)\n",
    "sequence_v = tokenizer.texts_to_sequences(sentences_v)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"len(index_of_words)\",len(index_of_words))\n",
    "\n",
    "padded_seq = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "padded_seq_t = pad_sequences(sequence_t , maxlen = max_seq_len )\n",
    "padded_seq_v = pad_sequences(sequence_v , maxlen = max_seq_len )\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "Y = to_categorical(Y)\n",
    "Y_t = to_categorical(Y_t)\n",
    "Y_v = to_categorical(Y_v)\n",
    "\n",
    "X_train=padded_seq\n",
    "Y_train=Y\n",
    "X_test=padded_seq_t\n",
    "Y_test=Y_t\n",
    "X_val=padded_seq_v\n",
    "Y_val=Y_v\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpxFlNvWJOOz"
   },
   "source": [
    "#CredRank Algorithm\n",
    "##Measuring User Credibility in Social Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8uHoYKHJdsq"
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     import flair\n",
    "# except:\n",
    "#     !pip install flair\n",
    "# import pickle\n",
    "# from flair.embeddings import TransformerDocumentEmbeddings\n",
    "# from flair.data import Sentence\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# selected_transformer_model=\"bert-base-cased\"\n",
    "# embedding = TransformerDocumentEmbeddings(selected_transformer_model)\n",
    "\n",
    "# def sentence_embedding(input_S):\n",
    "#     _input = Sentence(input_S)\n",
    "#     embedding.embed(_input)\n",
    "#     _input.get_embedding()\n",
    "#     _emb = _input.get_embedding()\n",
    "#     _emb = _emb.detach().numpy()\n",
    "#     return _emb\n",
    "\n",
    "# #########Train##############\n",
    "# all_emb = []\n",
    "# for _i,_sen in enumerate(df['Statement']):\n",
    "#     x = _sen\n",
    "#     _emb = sentence_embedding(x)\n",
    "#     print(_i,'=>',_emb.shape)\n",
    "#     all_emb.append(_emb)\n",
    "\n",
    "\n",
    "# outfile = data_path + \"train.npy\"\n",
    "# all_emb = np.array(all_emb)\n",
    "# np.save(outfile, all_emb)\n",
    "# clear_output()\n",
    "\n",
    "# #########Test##############\n",
    "# all_emb = []\n",
    "# for _i,_sen in enumerate(df_t['Statement']):\n",
    "#     x = _sen\n",
    "#     _emb = sentence_embedding(x)\n",
    "#     print(_i,'=>',_emb.shape)\n",
    "#     all_emb.append(_emb)\n",
    "\n",
    "\n",
    "# outfile = data_path + \"test.npy\"\n",
    "# all_emb = np.array(all_emb)\n",
    "# np.save(outfile, all_emb)\n",
    "\n",
    "# #########Valid##############\n",
    "# all_emb = []\n",
    "# for _i,_sen in enumerate(df_v['Statement']):\n",
    "#     x = _sen\n",
    "#     _emb = sentence_embedding(x)\n",
    "#     print(_i,'=>',_emb.shape)\n",
    "#     all_emb.append(_emb)\n",
    "\n",
    "\n",
    "# outfile = data_path + \"valid.npy\"\n",
    "# all_emb = np.array(all_emb)\n",
    "# np.save(outfile, all_emb)\n",
    "\n",
    "# clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5619,
     "status": "ok",
     "timestamp": 1598710626771,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "RAt6h1ZdoCQW",
    "outputId": "f83d320e-b06e-435d-9b83-a37a7ce6da59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_emb_train.shape=> (10240, 768)\n",
      "all_emb_test.shape=> (2551, 768)\n",
      "all_emb_valid.shape=> (2569, 768)\n",
      "tmp_all_emb.shape=> (15360, 768)\n"
     ]
    }
   ],
   "source": [
    "data_file = data_path + \"train.npy\"\n",
    "all_emb = np.load(data_file)\n",
    "print(\"all_emb_train.shape=>\",all_emb.shape)\n",
    "\n",
    "data_file = data_path + \"test.npy\"\n",
    "all_emb_t = np.load(data_file)\n",
    "print(\"all_emb_test.shape=>\",all_emb_t.shape)\n",
    "\n",
    "data_file = data_path + \"valid.npy\"\n",
    "all_emb_v = np.load(data_file)\n",
    "print(\"all_emb_valid.shape=>\",all_emb_v.shape)\n",
    "\n",
    "\n",
    "tmp_all_emb = np.concatenate((all_emb,all_emb_t,all_emb_v))\n",
    "print(\"tmp_all_emb.shape=>\",tmp_all_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ieYJ4WQdpUT"
   },
   "outputs": [],
   "source": [
    "# # # # from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# # # # from matplotlib import pyplot as plt\n",
    "\n",
    "# # # # linked = linkage(all_emb_t[:10,:], 'single')\n",
    "\n",
    "# # # # plt.figure(figsize=(10, 7))\n",
    "# # # # dendrogram (linked,\n",
    "# # # #             orientation='top',\n",
    "# # # #             distance_sort='descending',\n",
    "# # # #             show_leaf_counts=True)\n",
    "# # # # plt.show()\n",
    "\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster = AgglomerativeClustering(n_clusters=2,affinity='euclidean', linkage='ward')# affinity=\"cosine\", linkage='average')\n",
    "tmp_c=cluster.fit_predict(tmp_all_emb)\n",
    "\n",
    "outfile = data_path + \"clustering_result.npy\"\n",
    "np.save(outfile, tmp_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1623,
     "status": "ok",
     "timestamp": 1598710831923,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "MMVp7AiLDYbs",
    "outputId": "7f84d935-708a-439f-a26d-568e9e7656d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240\n",
      "[0 1]\n",
      "Counter({0: 7974, 1: 2266})\n",
      "2551\n",
      "[0 1]\n",
      "Counter({0: 2000, 1: 551})\n",
      "2569\n",
      "[0 1]\n",
      "Counter({0: 2013, 1: 556})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data_file = data_path + \"clustering_result.npy\"\n",
    "tmp_c = np.load(data_file)\n",
    "\n",
    "\n",
    "train_c = tmp_c[:len(all_emb)]\n",
    "print(len(train_c))\n",
    "print(np.unique(train_c))\n",
    "print(Counter(train_c))\n",
    "# print(Counter(Y_train.argmax(axis=1)))\n",
    "\n",
    "test_c = tmp_c[len(all_emb):len(all_emb)+len(all_emb_t)]\n",
    "print(len(test_c))\n",
    "print(np.unique(test_c))\n",
    "print(Counter(test_c))\n",
    "# print(Counter(Y_train.argmax(axis=1)))\n",
    "\n",
    "valid_c = tmp_c[len(all_emb)+len(all_emb_t):]\n",
    "print(len(valid_c))\n",
    "print(np.unique(valid_c))\n",
    "print(Counter(valid_c))\n",
    "\n",
    "\n",
    "n_of_m = Counter(train_c)\n",
    "cluster_weight = np.zeros(len(n_of_m))\n",
    "_tmp = np.array(list(n_of_m.values()))\n",
    "_for_div = np.sum(np.sqrt(_tmp))\n",
    "for _cluster in n_of_m:\n",
    "    _c_i = np.sqrt(n_of_m[_cluster])\n",
    "    # member_weight = np.sqrt(n_of_m[_cluster])/n_of_m[_cluster]\n",
    "    cluster_weight[_cluster] = (_c_i/_for_div)#*member_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYUTV5m3LV0B"
   },
   "source": [
    "#Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15998,
     "status": "ok",
     "timestamp": 1598710857330,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "Aq0kBzWtufEg",
    "outputId": "8270ad42-5e03-47a4-add3-af9c2754cb13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove*.zip\n",
    "# !mv glove*.100d.txt 'drive/My Drive/_MrAnsari/_data'\n",
    "\n",
    "f = open(data_path+'glove.6B.100d.txt')\n",
    "embedd_index = {}\n",
    "for line in f:\n",
    "    val = line.split()\n",
    "    word = val[0]\n",
    "    coff = np.asarray(val[1:],dtype = 'float')\n",
    "    embedd_index[word] = coff\n",
    "\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embedd_index))\n",
    "\n",
    "# a_file = open(data_path+\"data.pkl\", \"wb\")\n",
    "# pickle.dump(embedd_index, a_file)\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L-LYjN4NvkkU"
   },
   "outputs": [],
   "source": [
    "# a_file = open(data_path+\"data.pkl\", \"rb\")\n",
    "# output = pickle.load(a_file)\n",
    "# print('Found %s word vectors.' % len(embedd_index))\n",
    "\n",
    "embedd_index['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1178,
     "status": "ok",
     "timestamp": 1598711019370,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "se70c_6sP_VP",
    "outputId": "b35d8cc8-6669-40f1-f2cb-68439016a65d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7059, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(index_of_words) + 1, embed_num_dims))\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "\n",
    "for word,i in index_of_words.items():\n",
    "    temp = embedd_index.get(word)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp\n",
    "        \n",
    "#for plotting\n",
    "        tokens.append(embedding_matrix[i])\n",
    "        labels.append(word)\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9w3uUkwQlDT"
   },
   "outputs": [],
   "source": [
    "#TSNE algorithm used to visualize word embeddings having huge amount (100) dimensions\n",
    "\n",
    "def tsne():\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens[:200])\n",
    "    print(new_values.shape)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16,16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "tsne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6718,
     "status": "ok",
     "timestamp": 1598711254457,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "Sj5nPst4Q_3G",
    "outputId": "626eb920-e1a6-4e93-cb26-81cecdd69a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 100)      705900      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 50, 128)      84480       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           4128        global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            66          dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3)            0           dense_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            8           concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 794,582\n",
      "Trainable params: 794,582\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "input1 = Input(shape=(max_seq_len,))\n",
    "input2 = Input(shape=(1,))\n",
    "\n",
    "#Embedding layer before the actaul BLSTM \n",
    "embedd_layer = Embedding(len(index_of_words) + 1 , embed_num_dims , input_length = max_seq_len , weights = [embedding_matrix])(input1)\n",
    "BiLSTM = Bidirectional(LSTM(64 , return_sequences = True , dropout = 0.2 , recurrent_dropout = 0.2))(embedd_layer)\n",
    "GMP = GlobalMaxPooling1D()(BiLSTM)\n",
    "FC1 = Dense(32,activation = 'relu')(GMP)\n",
    "FC1_d = Dropout(0.2)(FC1)\n",
    "FC2 = Dense(2,activation = 'sigmoid')(FC1_d)\n",
    "_newinput = Concatenate()([FC2, input2])\n",
    "FC3 = Dense(2,activation = 'sigmoid')(_newinput)\n",
    "model = Model(inputs=[input1, input2], outputs=FC3)\n",
    "model.summary()\n",
    "\n",
    "_adam = Adam(lr = 0.01)\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = 'rmsprop' , metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mz9tgyJvwQ9a"
   },
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for _i in range(len(train_c)):\n",
    "    tmp_data = cluster_weight[train_c[_i]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(len(test_c)):\n",
    "    tmp_data = cluster_weight[test_c[_i]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(len(valid_c)):\n",
    "    tmp_data = cluster_weight[valid_c[_i]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 97583,
     "status": "ok",
     "timestamp": 1598711395364,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "zlPOTCytRXp5",
    "outputId": "cd175348-a7c8-432c-a96d-9ef00756347a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 9s 230ms/step - loss: 0.6856 - accuracy: 0.5617 - val_loss: 0.6926 - val_accuracy: 0.5200\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 8s 211ms/step - loss: 0.6768 - accuracy: 0.5617 - val_loss: 0.6860 - val_accuracy: 0.5200\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 8s 210ms/step - loss: 0.6728 - accuracy: 0.5617 - val_loss: 0.6860 - val_accuracy: 0.5200\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 8s 212ms/step - loss: 0.6695 - accuracy: 0.5617 - val_loss: 0.6844 - val_accuracy: 0.5200\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 8s 211ms/step - loss: 0.6665 - accuracy: 0.5636 - val_loss: 0.6795 - val_accuracy: 0.5356\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 8s 207ms/step - loss: 0.6619 - accuracy: 0.5738 - val_loss: 0.6800 - val_accuracy: 0.5333\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 8s 210ms/step - loss: 0.6569 - accuracy: 0.5757 - val_loss: 0.6753 - val_accuracy: 0.5403\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 8s 212ms/step - loss: 0.6502 - accuracy: 0.5794 - val_loss: 0.6788 - val_accuracy: 0.5380\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 8s 210ms/step - loss: 0.6431 - accuracy: 0.6123 - val_loss: 0.6798 - val_accuracy: 0.5815\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 8s 211ms/step - loss: 0.6391 - accuracy: 0.6517 - val_loss: 0.6779 - val_accuracy: 0.6049\n",
      "80/80 [==============================] - 3s 35ms/step - loss: 0.6752 - accuracy: 0.5990\n",
      "59.89807844161987\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([X_train,new_data],Y_train,epochs = 10 , batch_size = 256, validation_data = ([X_val,new_data_v],Y_val))\n",
    "\n",
    "result = model.evaluate([X_test,new_data_t],Y_test)\n",
    "print(result[1]*100)\n",
    "\n",
    "\n",
    "model_file = data_path + \"LSTM_Model.h5\"\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4340,
     "status": "ok",
     "timestamp": 1598711444047,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "1qaL4gamT4vl",
    "outputId": "a3f74b9c-36c4-4a78-a365-89506e6ecf16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 3s 36ms/step - loss: 0.6752 - accuracy: 0.5990\n",
      "59.89807844161987\n"
     ]
    }
   ],
   "source": [
    "model_file = data_path + \"LSTM_Model.h5\"\n",
    "model.load_weights(model_file)\n",
    "\n",
    "result = model.evaluate([X_test,new_data_t],Y_test)\n",
    "print(result[1]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18825,
     "status": "ok",
     "timestamp": 1598711466888,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "T6rIGGkh1lyU",
    "outputId": "bf898232-9602-4316-f609-fd3c422fc194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_Test_Accuracy=> 0.6060894386298763\n"
     ]
    }
   ],
   "source": [
    "y_pred_v  = model.predict([X_val,new_data_v])\n",
    "# print(\"Valid_Accuracy=>\",np.mean((y_pred_v.argmax(axis=1)==Y_val.argmax(axis=1))))\n",
    "\n",
    "y_pred_t  = model.predict([X_test,new_data_t])\n",
    "# print(\"Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==Y_test.argmax(axis=1))))\n",
    "\n",
    "y_pred  = model.predict([X_train,new_data])\n",
    "# print(\"Train_Accuracy=>\",np.mean((y_pred.argmax(axis=1)==Y_train.argmax(axis=1))))\n",
    "\n",
    "new_data = []\n",
    "for _i in range(y_pred.shape[0]):\n",
    "    tmp_data = [cluster_weight[train_c[_i]],y_pred[_i,1]]\n",
    "    new_data.append(tmp_data)\n",
    "new_data = np.array(new_data)\n",
    "\n",
    "new_data_t = []\n",
    "for _i in range(y_pred_t.shape[0]):\n",
    "    tmp_data = [cluster_weight[test_c[_i]],y_pred_t[_i,1]]\n",
    "    new_data_t.append(tmp_data)\n",
    "new_data_t = np.array(new_data_t)\n",
    "\n",
    "new_data_v = []\n",
    "for _i in range(y_pred_v.shape[0]):\n",
    "    tmp_data = [cluster_weight[valid_c[_i]],y_pred_v[_i,1]]\n",
    "    new_data_v.append(tmp_data)\n",
    "new_data_v = np.array(new_data_v)\n",
    "\n",
    "\n",
    "mode = 1\n",
    "if mode == 0:\n",
    "    new_train_data = new_data\n",
    "    new_train_label = Y_train.argmax(axis=1)\n",
    "\n",
    "    new_test_data = new_data_t\n",
    "    new_test_label = Y_test.argmax(axis=1)\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==new_test_label)))\n",
    "\n",
    "if mode == 1:\n",
    "    new_train_data = new_data_t[:1500,:]\n",
    "    new_train_label = Y_test.argmax(axis=1)[:1500]\n",
    "\n",
    "    new_test_data = new_data_t[1500:,:]\n",
    "    new_test_label = Y_test.argmax(axis=1)[1500:]\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)[1500:]==new_test_label)))\n",
    "\n",
    "if mode == 2:\n",
    "    new_train_data = new_data_v\n",
    "    new_train_label = Y_val.argmax(axis=1)\n",
    "\n",
    "    new_test_data = new_data_t\n",
    "    new_test_label = Y_test.argmax(axis=1)\n",
    "\n",
    "    print(\"New_Test_Accuracy=>\",np.mean((y_pred_t.argmax(axis=1)==new_test_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3484,
     "status": "ok",
     "timestamp": 1598711474225,
     "user": {
      "displayName": "Hossein Shahamat",
      "photoUrl": "",
      "userId": "06958071370778185224"
     },
     "user_tz": -270
    },
    "id": "aqqVxa0y8SWl",
    "outputId": "18a35c70-f9e4-46f6-8192-441631eaf9a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier \n",
      "Confusion matrix(test):\n",
      "[[225 271]\n",
      " [233 322]]\n",
      "Test_Accuracy=> 0.5204567078972407\n",
      "f1-score=>(test):0.5609756097560975\n",
      "ExtraTreesClassifier\n",
      "Confusion matrix(test):\n",
      "[[228 268]\n",
      " [232 323]]\n",
      "Test_Accuracy=> 0.5242626070409134\n",
      "f1-score=>(test):0.5636998254799302\n",
      "SVMClassifier\n",
      "Confusion matrix(test):\n",
      "[[297 199]\n",
      " [221 334]]\n",
      "Test_Accuracy=> 0.6003805899143673\n",
      "f1-score=>(test):0.613970588235294\n",
      "MLPClassifier\n",
      "Confusion matrix(test):\n",
      "[[207 289]\n",
      " [149 406]]\n",
      "Test_Accuracy=> 0.5832540437678402\n",
      "f1-score=>(test):0.6496000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf1 = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "      min_samples_split=2, random_state=0)\n",
    "clf1.fit(new_train_data, new_train_label)\n",
    "y_pred = clf1.predict(new_test_data)\n",
    "print(\"RandomForestClassifier \")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf2 = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "      min_samples_split=2, random_state=0)\n",
    "clf2.fit(new_train_data, new_train_label)\n",
    "y_pred = clf2.predict(new_test_data)\n",
    "print(\"ExtraTreesClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "clf3 = svm.SVC()\n",
    "clf3.fit(new_train_data, new_train_label)\n",
    "y_pred = clf3.predict(new_test_data)\n",
    "print(\"SVMClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf4 = MLPClassifier(random_state=1, max_iter=300).fit(new_train_data, new_train_label)\n",
    "y_pred = clf4.predict(new_test_data)\n",
    "print(\"MLPClassifier\")\n",
    "print('Confusion matrix(test):\\n{}'.format(confusion_matrix(new_test_label, y_pred)))\n",
    "print(\"Test_Accuracy=>\",np.mean((y_pred==new_test_label)))\n",
    "print('f1-score=>(test):{}'.format(f1_score(new_test_label, y_pred)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM/0nPgiTjrZljQ0CxVYsXg",
   "collapsed_sections": [],
   "mount_file_id": "1VEpVfCKAY_ej8Avr4rFHrPUlUmsXJXqL",
   "name": "Text7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
